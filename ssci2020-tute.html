<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Truyen Tran</title>










<meta content="en-us" http-equiv="Content-Language">
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">
<style type="text/css">
/* Layout-provided Styles */
ul.itemize {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;
}
</style>
<style>
body {
font-family: 'Abel', serif;
font-size: 12px;
}
</style></head><body>
<table style="border-collapse: collapse; width: 1038px; height: 1304px;" id="1" border="0" bordercolor="#111111" cellpadding="0" cellspacing="0">
<tbody>
<tr>
<td style="border-right: 1px solid;" v="" bgcolor="#000000">&nbsp;</td>
<td>&nbsp;</td>
<td style="vertical-align: top;" v="">&nbsp;</td>
</tr>
<tr>
<td rowspan="2" v="" style="border-width: 1px; border-right: 1px solid; vertical-align: top;">
<p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="deep learning" src="figs/deepLearningAI500.png" hspace="0"><br>
(Source: rdn-consulting)</p>
<p align="right">
</p>
<p align="right">
</p>
<p align="right">
</p>
<p align="right"><font size="5"><a href="index.html">Home</a></font>&nbsp;
&nbsp; </p>
<br>
<p align="justify"></p>
<p align="justify">&nbsp;</p>
<p align="justify"></p>
<p align="justify">&nbsp;</p>
<p align="justify">&nbsp;</p>
</td>
</tr>
<tr>
<td width="24">
<p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</td>
<td width="837">
<p>
<table style="text-align: left; margin-left: auto; margin-right: auto; width: 839px; height: 226px;" border="0" bordercolor="#000000" cellpadding="15" cellspacing="3">
<tbody>
<tr>
<td style="background-color: white;">
<p><font style="font-weight: bold; color: rgb(0, 102, 0);" size="+3">Deep Learning 1.0 and Beyond<br>
</font></p><p style="color: rgb(153, 153, 153);"><font style="color: rgb(0, 102, 0);" size="+2">A tutorial @IEEE SSCI 2020, Canberra, December 1st (Virtual).</font></p><p style="color: black; font-weight: bold;"><font size="+2">Slides (<a href="https://www2.slideshare.net/truyen/deep-learning-10-and-beyond-part-1">Part I</a>; <a href="https://www2.slideshare.net/truyen/deep-learning-10-and-beyond-part-2">Part II</a>)</font></p><p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;">Deep
Learning has taken the digital world by storm. As a general purpose
technology, it is now present in all walks of life. Although the
fundamental developments in methodology have been slowing down in the
past few years, applications are flourishing with major breakthroughs
in Computer Vision, NLP and Biomedical Sciences. The primary successes
can be attributed to the availability of large labelled data, powerful
GPU servers and programming frameworks, and advances in neural
architecture engineering. This combination enables rapid construction
of large, efficient neural networks that scale to the real world. But
the fundamental questions of unsupervised learning, deep reasoning, and
rapid contextual adaptation remain unsolved. We shall call what we
currently have Deep Learning 1.0, and the next possible breakthroughs
as Deep Learning 2.0</span><span style="font-weight: normal;">.</span></font></p>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Prerequisite</span>: <span style="font-style: italic;">the
tutorial assumes some familarity with deep learning.</span></span></font></p>
            <p style="color: black; font-weight: bold;"><font size="+2">Content</font></p>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;">This
tutorial goes through recent advances in methods and applications of
deep learning, and sketches a possible picture of the future. The
tutorial is broadly divided into two parts: Deep Learning 1.0 and Deep
Learning 20. In the first part, I start with the three classic
architectures: feed forward, recurrent and convolutional neural
networks. The tutorial then briefly reviews important concepts such as
attention, fast weight and architecture search. Then I cover the two
architectures that are at the frontier of current deep learning
research: the Transformer family and Graph Neural Networks. The last
topic is deep unsupervised learning, including the classic one like
autoencoder and the latest algorithms like BERT and self-supervised
techniques. In the second part of the tutorial, I discuss the reason
why we need to move beyond the current deep learning paradigm, and then
walk through emerging topics that may shape the next phase of research.
In particular, I present a general dual-process cognitive architecture
which can be implemented as a neural system. One of the main components
in the architecture is the memory sub-system, which extends the
capacity of the current fast parallel inference neural models. The
capacity is realised through reasoning engines that carry out
deliberative sequential inference. Finally, I cover the emerging topic
of neural theory of mind, which is concerned with the social dimension
of multi-agent learning systems.<br>
            </span></font></p>
<p style="color: black; font-weight: bold;"><font size="+2">Structure:</font></p><p style="color: black; font-weight: bold; margin-left: 40px;"><font size="+2">Part I: Deep Learning 1.0 (90 mins)<br>
</font></p>
            <ul style="margin-left: 40px;">
              <li><font size="+2"><span style="font-weight: normal;">Introduction (10 mins)</span><span style="font-weight: normal;"></span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Classic models and concepts (20 mins)</span><br style="font-weight: normal;"></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Transformers (20 mins)</span><br style="font-weight: normal;"></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Graph neural networks (20 mins)</span><br style="font-weight: normal;"></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Deep unsupervised learning (20 mins)</span><br style="font-weight: normal;"></font></li>
            </ul>
<p style="color: black; font-weight: bold; margin-left: 40px;"><font size="+2">Part II: Deep Learning 2.0 (90 mins)</font><br style="font-weight: normal;"></p>
            <ul style="margin-left: 40px;">
<li><font size="+2"><span style="font-weight: normal;">Introduction (10 mins)</span><span style="font-weight: normal;"></span></font></li><li><font size="+2"><span style="font-weight: normal;">A system view (20 mins)</span><br style="font-weight: normal;"></font></li><li><font size="+2"><span style="font-weight: normal;">Neural memories (20 mins)</span><br style="font-weight: normal;"></font></li><li><font size="+2"><span style="font-weight: normal;">Neural reasoning (20 mins)</span><br style="font-weight: normal;"></font></li><li><font size="+2"><span style="font-weight: normal;">Neural theory of mind (20 mins)</span></font></li>
            </ul>
<ul style="color: black;">
</ul>
</td>
</tr>
</tbody>
</table>
</p>
<hr><br>
<p style="margin-left: 40px;" align="justify"><font style="font-weight: bold;" size="5">References</font></p>
      <ol style="margin-left: 40px;">
        <li>
          <p><font size="5">Anonymous, “Neural spatio-temporal
reasoning with object-centric self-supervised learning”,
https://openreview.net/pdf?id=rEaz5uTcL6Q</font></p>
        </li>
        <li>
          <p><font size="5">Bello, Irwan, et al. "Neural optimizer search with reinforcement learning." <span style="font-style: italic;">ICML</span> (2017).</font></p>
        </li>
        <li>
          <p><font size="5">Bengio, Yoshua, Aaron Courville, and Pascal Vincent. "Representation learning: A review and new perspectives."&nbsp;<span style="font-style: italic;">IEEE transactions on pattern analysis and machine intelligence</span>&nbsp;35.8 (2013): 1798-1828.</font></p>
        </li>
        <li>
          <p><font size="5">Bottou, Léon. "From machine learning to machine reasoning."&nbsp;<span style="font-style: italic;">Machine learning</span>&nbsp;94.2 (2014): 133-149.</font></p>
        </li>
        <li>
          <p><font size="5">Dehghani, Mostafa, et al. "Universal Transformers."&nbsp;<span style="font-style: italic;">International Conference on Learning Representations.</span> 2018.</font></p>
        </li>
        <li>
          <p><font size="5">Kien Do, Truyen Tran, and Svetha Venkatesh. "Graph Transformation Policy Network for Chemical Reaction Prediction."&nbsp;<span style="font-style: italic;">KDD’19</span>.</font></p>
        </li>
        <li>
          <p><font size="5">Kien Do, Truyen Tran, Svetha Venkatesh, “Learning deep matrix representations”, &nbsp;<span style="font-style: italic;">arXiv preprint</span> arXiv:1703.01454</font></p>
        </li>
        <li>
          <p><font size="5">Gilmer, Justin, et al. "Neural message passing for quantum chemistry." <span style="font-style: italic;">ICML</span> (2017).</font></p>
        </li>
        <li>
          <p><font size="5">Ha, David, Andrew Dai, and Quoc V. Le. "Hypernetworks." <span style="font-style: italic;">ICLR</span> (2017).</font></p>
        </li>
        <li>
          <p><font size="5">Heskes, Tom. "Stable fixed points of loopy belief propagation are local minima of the bethe free energy."&nbsp;<span style="font-style: italic;">Advances in neural information processing systems.</span> 2003.</font></p>
        </li>
        <li>
          <p><font size="5">Hudson, Drew A., and Christopher D. Manning. "Compositional attention networks for machine reasoning." <span style="font-style: italic;">ICLR</span> (2018).</font></p>
        </li>
        <li>
          <p><font size="5">Karras, T., Aila, T., Laine, S., &amp;
Lehtinen, J. "Progressive growing of GANs for improved quality,
stability, and variation". <span style="font-style: italic;">ICLR</span> (2018).</font></p>
        </li>
        <li>
          <p><font size="5">Khardon, Roni, and Dan Roth. "Learning to reason."&nbsp;<span style="font-style: italic;">Journal of the ACM</span> (JACM)&nbsp;44.5 (1997): 697-725.</font></p>
        </li>
        <li>
          <p><font size="5">Hung Le, Truyen Tran, Svetha Venkatesh, “Self-attentive associative memory”,<span style="font-style: italic;"> ICML</span> (2020).</font></p>
        </li>
        <li>
          <p><font size="5">Hung Le, Truyen Tran, Svetha Venkatesh, “Neural stored-program memory”, <span style="font-style: italic;">ICLR</span> (2020).</font></p>
        </li>
        <li>
          <p><font size="5">Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran, “Dynamic Language Binding in Relational Visual Reasoning”, <span style="font-style: italic;">IJCAI </span>(2020).</font></p>
        </li>
        <li>
          <p><font size="5">Le-Khac, Phuc H., Graham Healy, and Alan F. Smeaton. "Contrastive Representation Learning: A Framework and Review."&nbsp;<span style="font-style: italic;">arXiv preprint</span> arXiv:2010.05113&nbsp;(2020).</font></p>
        </li>
        <li>
          <p><font size="5">Liu, Xiao, et al. "Self-supervised learning: Generative or contrastive."&nbsp;<span style="font-style: italic;">arXiv preprint</span> arXiv:2006.08218&nbsp;(2020).</font></p>
        </li>
        <li>
          <p><font size="5">Marcus, Gary. "Deep learning: A critical appraisal."&nbsp;<span style="font-style: italic;">arXiv preprint</span> arXiv:1801.00631&nbsp;(2018).</font></p>
        </li>
        <li>
          <p><font size="5">Mao, Jiayuan, et al. "The Neuro-Symbolic
Concept Learner: Interpreting Scenes, Words, and Sentences From Natural
Supervision."&nbsp;<span style="font-style: italic;">International Conference on Learning Representations</span> (2019).</font></p>
        </li>
        <li>
          <p><font size="5">Nguyen, Dung, et al. "Theory of Mind with Guilt Aversion Facilitates Cooperative Reinforcement Learning."&nbsp;<span style="font-style: italic;">Asian Conference on Machine Learning</span>. (2020)</font></p>
        </li>
        <li>
          <p><font size="5">Penmatsa, Aravind, Kevin H. Wang, and Eric
Gouaux. "X-ray structure of dopamine transporter elucidates
antidepressant mechanism."&nbsp;<span style="font-style: italic;">Nature&nbsp;</span>503.7474 (2013): 85-90.</font></p>
        </li>
        <li>
          <p><font size="5">Pham, Trang, et al. "Column Networks for Collective Classification."&nbsp;<span style="font-style: italic;">AAAI</span> (2017).</font></p>
        </li>
        <li>
          <p><font size="5">Ramsauer, Hubert, et al. "Hopfield networks is all you need."&nbsp;<span style="font-style: italic;">arXiv preprint</span> arXiv:2008.02217&nbsp;(2020).</font></p>
        </li>
        <li>
          <p><font size="5">Rabinowitz, Neil C., et al. "Machine theory of mind." <span style="font-style: italic;">ICML</span> (2018).</font></p>
        </li>
        <li>
          <p><font size="5">Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. "End-to-end memory networks." <span style="font-style: italic;">Advances in neural information processing systems</span> (2015).</font></p>
        </li>
        <li>
          <p><font size="5">Tay, Yi, et al. "Efficient transformers: A survey."&nbsp;<span style="font-style: italic;">arXiv preprint</span> arXiv:2009.06732&nbsp;(2020).</font></p>
        </li>
        <li>
          <p><font size="5">Xie, Tian, and Jeffrey C. Grossman.
"Crystal Graph Convolutional Neural Networks for an Accurate and
Interpretable Prediction of Material Properties."&nbsp;<span style="font-style: italic;">Physical review letters&nbsp;</span>120.14 (2018): 145301.</font></p>
        </li>
        <li>
          <p><font size="5">You, Jiaxuan, et al. "GraphRNN: Generating realistic graphs with deep auto-regressive models."&nbsp;<span style="font-style: italic;">ICML</span>&nbsp;(2018).</font></p>
        </li>
      </ol>
</td></tr></tbody></table><p align="justify">&nbsp;</p>
</body></html>