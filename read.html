<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Truyen Tran</title>







<meta content="en-us" http-equiv="Content-Language">
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">
<style>
body {
font-family: 'Abel';
font-size: 100px;
}
</style></head><body>
<table style="border-collapse: collapse; width: 923px; height: 500px;" id="1" border="0" bordercolor="#111111" cellpadding="0" cellspacing="0">
<tbody>
<tr>
<td style="border-right: 1px solid;" v="" bgcolor="#000000">&nbsp;</td>
<td style="width: 200px;">&nbsp;</td>
<td style="vertical-align: top;" v="">&nbsp;</td>
</tr>
<tr>
<td rowspan="2" v="" style="border-width: 1px; border-right: 1px solid; vertical-align: top;">
<p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="generated digits" src="http://rdn-consulting.com/blog/wp-content/uploads/2015/12/deepLearningAI500.png" hspace="0"><br>
</p>
<p align="right">[Source:&nbsp;rdn-consulting]</p>
<p align="right"></p>
<p align="right">
</p>
<p align="right"><font size="5"><a href="index.html">Home</a></font>&nbsp;
&nbsp; </p>
<br>
<p align="justify"></p>
<p align="justify">&nbsp;</p>
<p align="justify"></p>
<p align="justify">&nbsp;</p>
<p align="justify">&nbsp;</p>
</td>
</tr>
<tr>
<td style="width: 24px;">
<p></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</td>
<td style="vertical-align: top; width: 80%;">
      <h1>Reading Club on AI</h1>
      <big>(Friday, 2-4PM, Room: ka5.213 Waurn Ponds)</big><br>
      <br>
<ul><li><big>--/--/----: Information geometry&nbsp; (Cat Le)</big></li><li><big>--/--/----: Deep Boltzmann machines&nbsp; (Cat Le)</big></li><li><big>--/--/----: Quantum machine learning (Truyen Tran)</big></li><li><big>--/--/----: Organizing the world's knowledge (Truyen Tran)</big></li><li><big>--/--/----: Deep learning for materials science&nbsp;(Truyen Tran)</big></li><li><big>--/--/----: Continual learning</big></li><li><big>--/--/----: Bayesian optimization (Hung Tran)</big></li><li><big>--/--/----: Few-shot learning</big></li><li><big>--/--/----: <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference">ICLR-18</a> key papers</big></li><li><big>--/--/----: <a href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017">NIPS-17</a> key papers</big></li>
        <li><big>--/--/----</big><big>: Deep learning for biomedicine II (Truyen Tran)</big></li><li><big>--/--/----</big><big>: MANN III (Hung Le)</big></li>
        <li><big>--/--/----</big><big>: GAN II (Tung Hoang)</big></li><li><big>09/03/2018</big><big>: The future of deep learning IV: near/medium-term (Truyen Tran)</big></li><li style="font-weight: bold;"><big>02/03/2018</big><big>: The future of deep learning III: near/medium-term (Truyen Tran)</big></li>
<li style="color: rgb(102, 102, 102);"><big>23/02/2018</big><big>: The future of deep learning II: near/medium-term (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>09/02/2018: Graph modeling II: Molecular models (Trang Pham)</big></li><li style="color: rgb(102, 102, 102);"><big>02/02/2018: RNN (Hung Le)</big></li><li style="color: rgb(102, 102, 102);"><big>19/01/2018: The future of deep learning I:&nbsp; long-term (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>01/12/2017: Deep learning for biomedicine I (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>24/11/2017: Bayesian deep learning (Tung Hoang)</big></li>
<li style="color: rgb(102, 102, 102);"><big>10/11/2017: Action recognition on video (Vuong Le)</big></li><li style="color: rgb(102, 102, 102);"><big>03/11/2017: Capsules (Trang Pham)</big></li><li style="color: rgb(102, 102, 102);"><big>27/10/2017: AAAI'18 reviews (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>20/10/2017: Text + knowledge graphs for question answering (Kien Do)</big></li><li style="color: rgb(102, 102, 102);"><big>13/10/2017: Learning intuitive physics from video (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>29/09/2017: Object detection (Budhaditya Saha)</big></li><li style="color: rgb(102, 102, 102);"><big>22/09/2017: MANN II (Hung Le)</big></li><li style="color: rgb(102, 102, 102);"><big>15/09/2017:&nbsp;GAN I (Tung Hoang)</big></li><li style="color: rgb(102, 102, 102);"><big>08/09/2017:<a href="https://www.ijcai.org/proceedings/2017/"> IJCAI-17</a> key papers (Kien Do, Trang Pham &amp; Phuoc Nguyen)</big></li><li style="color: rgb(102, 102, 102);"><big>01/09/2017:<a href="https://2017.icml.cc/Conferences/2017"> ICML-17</a> review &amp; key papers (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>04/08/2017: Meta-learning (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>28/07/2017: Attention mechanisms (Trang Pham)</big></li><li style="color: rgb(102, 102, 102);"><big>21/07/2017: Graph modeling I (Kien Do)</big></li><li><big><big><small><span style="color: rgb(102, 102, 102);">14/07/2017: Memory-augmented neural nets (MANN) I (Phuoc Nguyen)</span></small><br>
          </big></big></li></ul><br><font style="font-weight: bold;" size="+2">Capsules<br>
</font>
      <ol>
        <li><font size="+1"><a href="https://arxiv.org/abs/1710.09829">Dynamic Routing Between Capsules</a> (Hinton's group, NIPS'17)</font></li>
        <li><font size="+1"><a href="https://openreview.net/forum?id=HJWLfGWRb">Matrix capsules with EM routing</a> (Hinton's group, ICLR'18)</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Recurrent nets<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1609.01704">Learning nested hierarchy in RNN</a> (Bengio's group, 2016), learn the temporal hierarchy itself</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>
(2016), learn to generate weight matrices, meta-learning, fast synaptic
memory (Hinton), or learn network to learn networks</font></li><li><font size="+1"><a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf">Higherarchical attention model for document</a>s (NAACL, 2016), attention model</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1603.08983">Adaptive time computation</a> (2016), learn the determine the length of computation</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Highway networks</a> (NIPS, 2015), open the gate</font></li></ol><font style="font-weight: bold;" size="+2">Optimization<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1502.03167">Batch normalization</a> (ICLR, 2015), a technique for faster learning via a better conditioning for optimization through normalizing data</font></li><li><font size="+1"><a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">Dropout</a> (JMLR, 2014), most important discovery in years</font></li></ol><font style="font-weight: bold;" size="+2">Multi-agent systems<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1605.07736">Learning communication in multiagent</a> (NIPS, 2016), learn to share states between agents</font></li></ol><br><font style="font-weight: bold;" size="+2">Intuitive Physics<br></font><ol><li><font size="+1"><a href="http://papers.nips.cc/paper/6417-interaction-networks-for-learning-about-objects-relations-and-physics">Interaction networks for learning about objects, relations and physics</a> (NIPS, 2016), simple model for object interactions over time. It's <a href="https://arxiv.org/abs/1706.01433">end-to-end version</a> on videos (2017).</font></li><li><font size="+1"><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitlebuilding-machines-that-learn-and-think-like-peoplediv/A9535B1D745A0377E16C590E14B94993">Building machines that learn and think like people</a> (Lake et al, 2016), arguing for cognitively informed machine learning.</font></li></ol><font style="font-weight: bold;" size="+2">Memory<br></font><ol><li><font size="+1"><a href="https://arxiv.org/pdf/1610.06258v2.pdf">Using Fast Weights to Attend to the Recent Past</a> (Ba &amp; Hinton, 2016), Hebbian learning with fast synapse plasticity.</font> <font size="+1">Related to <a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>. </font></li><li><font size="+1"><a href="http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature20101.pdf">Hybrid computing using a neural network with dynamic external memory</a> (DeepMind, 2016), role of external memory. Early version: </font><font size="+1"><a href="http://arxiv.org/abs/1410.5401">Neural Turing machine</a> (2014).</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1506.07285">Ask me anything: dynamic memory networks </a>(ICML, 2016), a cool way to do question answering using memory</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks">End-to-end memory networks</a> (NIPS, 2015), role of memory</font></li></ol><font style="font-weight: bold;" size="+2">Continual learning<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1606.04671">Progressive neural networks</a> (2016), <a href="http://dl.acm.org/citation.cfm?id=1553380">curriculumn learning</a> (ICML, 2009),<a href="http://www.sciencedirect.com/science/article/pii/0010027793900584"> learning to start "small"</a> (1992)</font></li></ol><font style="font-weight: bold;" size="+2">Meta-learning<br></font><ol><li><font size="+1"><a href="https://arxiv.org/abs/1605.06065">Meta-Learning with Memory-Augmented Neural Networks</a> (ICML, 2016)</font></li></ol><font style="font-weight: bold;" size="+2">GAN/VAE/Implicit models<br></font><ol><li><font size="+1"><a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a> (ICML, 2017), a key development of GAN. It's <a href="https://arxiv.org/abs/1704.00028">improved version</a> (ICML, 2017).</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5423-generative-adversarial">Generative Adversarial Nets</a>&nbsp;(NIPS, 2014), a totally new way of generating high quality data</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1312.6114">Variational Auto-Encoders</a> (NIPS, 2014), a new powerful way to learn a generative model</font></li><li><font size="+1"><a href="http://dustintran.com/blog/my-qualifying-exam-oral">A list compiled by Dustin Tran</a> (2017)<br></font></li></ol><font style="font-weight: bold;" size="+2">Graph modeling<br></font><ol><li><font size="+1">Convolutional graphs (multiple papers: this, this, this and this; 2015, 2016), this is what column net (version 2016) is about</font></li></ol><font style="font-weight: bold;" size="+2">Reinforcement learning<br></font><ol><li><font size="+1"><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">AlphaGo</a> (Nature, 2016), the breakthrough of the decade. <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">AlphaGo Zero</a> (Nature, 2017), the player that learns from absolute zeros.</font></li></ol><font style="font-weight: bold;" size="+2">AI for physical sciences &amp; biomedicine<br></font><ol><li><font size="+1"><a href="http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html">DeepBind</a> (Nature BioTech, 2015), motifs with CNN</font></li><li><font size="+1"><a href="https://truyentran.github.io/acml17-tute.html">Truyen's ACML'17 tutorial</a><br></font></li></ol><br><font style="font-weight: bold;" size="+2">AI for NLP<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1609.08144">Google Neural Machine Translation</a> (2016), <a href="http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces">seq2seq</a> (NIPS, 2014). Beginning and an end of Neural Machine Translation.</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1507.07998">Paragraph2vec</a> (ICML, 2014), context ID embedding</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1502.01710">CNNs for chars in text</a> (2015), end-to-end text classification at character level<br></font></li><li><font size="+1"><a href="https://arxiv.org/abs/1103.0398">Natural Language Processing (Almost) from Scratch</a> (JMLR, 2011), CNN for text for the first time<br></font></li><li><font size="+1"><a href="http://www.jmlr.org/papers/v3/bengio03a.html">Neural nets for language model</a> (JMLR, 2003), the first of its kind</font></li></ol><br><font style="font-weight: bold;" size="+2">AI for Vision<br></font><ol><li><font size="+1"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-w">ImageNet Classification with Deep Convolutional Neural Networks</a> (NIPS, 2012), the new revolution in vision</font></li><li><font size="+1">DeepFace: Closing the Gap to Human-Level Performance in Face Verification (CVPR, 2014), deep verification is solved.<br></font></li></ol><font style="font-weight: bold;" size="+2">First wave of unsupervised learning (2006-2011)<br></font><ol><li><font size="+1"><a href="http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines">Multiprediction DBM</a> (NIPS, 2013), <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html">CRF as RNN</a> (CVPR, 2015), mean-fields (~1998)</font></li><li><font size="+1"><a href="https://www.ncbi.nlm.nih.gov/pubmed/16873662">Reducing the Dimensionality of Data with Neural Networks</a> (Science, 2006), the new revolution of deep learning<br></font></li></ol><font style="font-weight: bold;" size="+2">Other reading groups<br></font><ol><li><font size="+1"><a href="https://casmls.github.io/schedule/">Columbia</a><br></font></li></ol></td>
</tr>
</tbody>
</table>
<br>
</body></html>