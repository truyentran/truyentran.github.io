<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head><meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"><title>Reading club</title></head>
<body><h1>Reading Club on AI</h1><ol><li><font size="+1">29/8/2017: NIPS-17 key papers</font></li><li><font size="+1">22/8/2017: Recent advances in memory nets (Hung Le)</font></li><li><font size="+1">15/8/2017: GAN (Tung Hoang)</font></li><li><font size="+1">8/8/2017: IJCAI-17 key papers (Kien Do, Trang Pham &amp; Phuoc Nguyen)</font></li><li><font size="+1">1/9/2017: ICML-17 review &amp; key papers (Truyen Tran)</font></li><li><font size="+1">4/8/2018: Meta-learning (Truyen Tran)</font></li><li><font size="+1">28/7/2018: Attention mechanisms (Trang Pham)</font></li><li><font size="+1">21/7/2018: Graph modeling (Kien Do)</font></li><li><font size="+1">20/7/2018: Memory-augmented neural nets (Phuoc Nguyen)<br></font></li></ol><br><font style="font-weight: bold;" size="+2">References</font><ol><li><font size="+1"><a href="https://arxiv.org/pdf/1610.06258v2.pdf">Using Fast Weights to Attend to the Recent Past</a> (Ba &amp; Hinton, 2016), Hebbian learning with fast synapse plasticity.</font> <font size="+1">Related to <a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>. </font></li><li><font size="+1"><a href="http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature20101.pdf">Hybrid computing using a neural network with dynamic external memory</a> (DeepMind, 2016), role of external memory</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1609.01704">Learning nested hierarchy in RNN</a> (Bengio's group, 2016), learn the temporal hierarchy itself</font></li><li><font size="+1"><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">AlphaGo</a> (Nature, 2016), the breakthrough of the decade</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1312.6114">Variational Auto-Encoders</a> (NIPS, 2014), a new powerful way to learn a generative model</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1502.03167">Batch normalization</a> (ICLR, 2015), a technique for faster learning via a better conditioning for optimization through normalizing data</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>
(2016), learn to generate weight matrices, meta-learning, fast synaptic
memory (Hinton), or learn network to learn networks</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1609.08144">Google Neural Machine Translation</a> (2016), <a href="http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces">seq2seq</a> (NIPS, 2014). Beginning and an end of Neural Machine Translation.</font></li><li><font size="+1">Convolutional graphs (multiple papers: this, this, this and this; 2015, 2016), this is what column net (version 2016) is about</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1605.07736">Learning communication in multiagent</a> (NIPS, 2016), learn to share states between agents</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1606.04671">Progressive neural networks</a> (2016), <a href="http://dl.acm.org/citation.cfm?id=1553380">curriculumn learning</a> (ICML, 2009),<a href="http://www.sciencedirect.com/science/article/pii/0010027793900584"> learning to start "small"</a> (1992)</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5423-generative-adversarial">Generative Adversarial Nets</a>&nbsp;(NIPS, 2014), a totally new way of generating high quality data</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines">Multiprediction DBM</a> (NIPS, 2013), <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html">CRF as RNN</a> (CVPR, 2015), mean-fields (~1998)</font></li><li><font size="+1"><a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf">Higherarchical attention model for document</a>s (NAACL, 2016), attention model</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1603.08983">Adaptive time computation</a> (2016), learn the determine the length of computation</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1502.01710">CNNs for chars in text</a> (2015), end-to-end text classification at character level<br></font></li><li><font size="+1"><a href="http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html">DeepBind</a> (Nature BioTech, 2015), motifs with CNN</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1507.07998">Paragraph2vec</a> (ICML, 2014), context ID embedding</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1506.07285">Ask me anything: dynamic memory networks </a>(ICML, 2016), a cool way to do question answering using memory</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Highway networks</a> (NIPS, 2015), open the gate</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks">End-to-end memory networks</a> (NIPS, 2015), role of memory</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1410.5401">Neural Turing machine</a> (NIPS, 2014), role of memory in execution</font></li><li><font size="+1"><a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">Dropout</a> (JMLR, 2014), most important discovery in years</font></li><li><font size="+1">DeepFace: Closing the Gap to Human-Level Performance in Face Verification (CVPR, 2014), deep verification is solved.<br></font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-w">ImageNet Classification with Deep Convolutional Neural Networks</a> (NIPS, 2012), the new revolution in vision</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1103.0398">Natural Language Processing (Almost) from Scratch</a> (JMLR, 2011), CNN for text for the first time<br></font></li><li><font size="+1"><a href="https://www.ncbi.nlm.nih.gov/pubmed/16873662">Reducing the Dimensionality of Data with Neural Networks</a> (Science, 2006), the new revolution of deep learning<br></font></li><li><font size="+1"><a href="http://www.jmlr.org/papers/v3/bengio03a.html">Neural nets for language model</a> (JMLR, 2003), the first of its kind<br></font></li></ol></body></html>