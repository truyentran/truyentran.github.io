<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html><head><meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"><title>Reading club</title></head>
<body><h1>Reading Club on AI</h1><ol><li><font size="+1">--/--/----: Vision in the real-world (Vuong Le)</font></li><li><font size="+1">--/--/----: Organizing the world's knowledge (Truyen Tran)</font></li><li><font size="+1">--/--/----: Deep learning for physical sciences&nbsp;</font><font size="+1"> (Truyen Tran, Trang Pham)</font></li><li><font size="+1">--/--/----</font><font size="+1">: Continual learning</font></li><li><font size="+1">--/--/----</font><font size="+1">: Bayesian optimization</font></li><li><font size="+1">--/--/----</font><font size="+1">: Few-shot learning</font></li><li><font size="+1">--/--/----</font><font size="+1">: ICLR-18 key papers</font></li><li><font size="+1">--/--/----</font><font size="+1">: NIPS-17 key papers</font></li><li><font size="+1">--/--/----</font><font size="+1">: </font><font size="+1">Deep learning for biomedicine (Truyen Tran)</font></li><li><font size="+1">03/11/2017</font><font size="+1">:&nbsp;</font><font size="+1">Convolutional graphs (Trang Pham)</font></li><li><font size="+1">27/10/2017</font><font size="+1">: AAAI'18 reviews</font><font size="+1"> (Truyen Tran)</font><font size="+1"></font></li><li><font size="+1">20/10/2017: Text + knowledge graphs for question answering (Kien Do)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">13/10/2017: Learning intuitive physics from video (Truyen Tran)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">29/09/2017: Object detection (Budhaditya Saha)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">22/09/2017: MANN II (Hung Le)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">15/09/2017:&nbsp;GAN I (Tung Hoang)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">08/09/2017: IJCAI-17 key papers (Kien Do, Trang Pham &amp; Phuoc Nguyen)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">01/09/2017: ICML-17 review &amp; key papers (Truyen Tran)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">04/08/2017: Meta-learning (Truyen Tran)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">28/07/2017: Attention mechanisms (Trang Pham)</font></li><li style="color: rgb(102, 102, 102);"><font size="+1">21/07/2017: Graph modeling (Kien Do)</font></li><li><font size="+1"><span style="color: rgb(102, 102, 102);">14/07/2017: Memory-augmented neural nets (MANN) I (Phuoc Nguyen)</span><br></font></li></ol><br><font style="font-weight: bold;" size="+2">Recurrent nets<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1609.01704">Learning nested hierarchy in RNN</a> (Bengio's group, 2016), learn the temporal hierarchy itself</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>
(2016), learn to generate weight matrices, meta-learning, fast synaptic
memory (Hinton), or learn network to learn networks</font></li><li><font size="+1"><a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf">Higherarchical attention model for document</a>s (NAACL, 2016), attention model</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1603.08983">Adaptive time computation</a> (2016), learn the determine the length of computation</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Highway networks</a> (NIPS, 2015), open the gate</font></li></ol><font style="font-weight: bold;" size="+2">Optimization<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1502.03167">Batch normalization</a> (ICLR, 2015), a technique for faster learning via a better conditioning for optimization through normalizing data</font></li><li><font size="+1"><a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">Dropout</a> (JMLR, 2014), most important discovery in years</font></li></ol><font style="font-weight: bold;" size="+2">Multi-agent systems<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1605.07736">Learning communication in multiagent</a> (NIPS, 2016), learn to share states between agents</font></li></ol><br><font style="font-weight: bold;" size="+2">Intuitive Physics<br></font><ol><li><font size="+1"><a href="http://papers.nips.cc/paper/6417-interaction-networks-for-learning-about-objects-relations-and-physics">Interaction networks for learning about objects, relations and physics</a> (NIPS, 2016), simple model for object interactions over time. It's <a href="https://arxiv.org/abs/1706.01433">end-to-end version</a> on videos (2017).</font></li><li><font size="+1"><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitlebuilding-machines-that-learn-and-think-like-peoplediv/A9535B1D745A0377E16C590E14B94993">Building machines that learn and think like people</a> (Lake et al, 2016), arguing for cognitively informed machine learning.</font></li></ol><font style="font-weight: bold;" size="+2">Memory<br></font><ol><li><font size="+1"><a href="https://arxiv.org/pdf/1610.06258v2.pdf">Using Fast Weights to Attend to the Recent Past</a> (Ba &amp; Hinton, 2016), Hebbian learning with fast synapse plasticity.</font> <font size="+1">Related to <a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>. </font></li><li><font size="+1"><a href="http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature20101.pdf">Hybrid computing using a neural network with dynamic external memory</a> (DeepMind, 2016), role of external memory. Early version: </font><font size="+1"><a href="http://arxiv.org/abs/1410.5401">Neural Turing machine</a> (2014).</font></li><li><font size="+1"><a href="https://arxiv.org/abs/1506.07285">Ask me anything: dynamic memory networks </a>(ICML, 2016), a cool way to do question answering using memory</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks">End-to-end memory networks</a> (NIPS, 2015), role of memory</font></li></ol><font style="font-weight: bold;" size="+2">Continual learning<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1606.04671">Progressive neural networks</a> (2016), <a href="http://dl.acm.org/citation.cfm?id=1553380">curriculumn learning</a> (ICML, 2009),<a href="http://www.sciencedirect.com/science/article/pii/0010027793900584"> learning to start "small"</a> (1992)</font></li></ol><font style="font-weight: bold;" size="+2">Meta-learning<br></font><ol><li><font size="+1"><a href="https://arxiv.org/abs/1605.06065">Meta-Learning with Memory-Augmented Neural Networks</a> (ICML, 2016)</font></li></ol><font style="font-weight: bold;" size="+2">GAN/VAE<br></font><ol><li><font size="+1"><a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a> (ICML, 2017), a key development of GAN. It's <a href="https://arxiv.org/abs/1704.00028">improved version</a> (ICML, 2017).</font></li><li><font size="+1"><a href="http://papers.nips.cc/paper/5423-generative-adversarial">Generative Adversarial Nets</a>&nbsp;(NIPS, 2014), a totally new way of generating high quality data</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1312.6114">Variational Auto-Encoders</a> (NIPS, 2014), a new powerful way to learn a generative model</font></li></ol><font style="font-weight: bold;" size="+2">Graph modeling<br></font><ol><li><font size="+1">Convolutional graphs (multiple papers: this, this, this and this; 2015, 2016), this is what column net (version 2016) is about</font></li></ol><font style="font-weight: bold;" size="+2">AI for physical sciences<br></font><ol><li><font size="+1"><a href="http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html">DeepBind</a> (Nature BioTech, 2015), motifs with CNN</font></li></ol><br><font style="font-weight: bold;" size="+2">Reinforcement learning<br></font><ol><li><font size="+1"><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">AlphaGo</a> (Nature, 2016), the breakthrough of the decade. <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">AlphaGo Zero</a> (Nature, 2017), the player that learns from absolute zeros.</font></li></ol><font style="font-weight: bold;" size="+2">NLP<br></font><ol><li><font size="+1"><a href="http://arxiv.org/abs/1609.08144">Google Neural Machine Translation</a> (2016), <a href="http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces">seq2seq</a> (NIPS, 2014). Beginning and an end of Neural Machine Translation.</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1507.07998">Paragraph2vec</a> (ICML, 2014), context ID embedding</font></li><li><font size="+1"><a href="http://arxiv.org/abs/1502.01710">CNNs for chars in text</a> (2015), end-to-end text classification at character level<br></font></li><li><font size="+1"><a href="https://arxiv.org/abs/1103.0398">Natural Language Processing (Almost) from Scratch</a> (JMLR, 2011), CNN for text for the first time<br></font></li><li><font size="+1"><a href="http://www.jmlr.org/papers/v3/bengio03a.html">Neural nets for language model</a> (JMLR, 2003), the first of its kind</font></li></ol><br><font style="font-weight: bold;" size="+2">Vision<br></font><ol><li><font size="+1"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-w">ImageNet Classification with Deep Convolutional Neural Networks</a> (NIPS, 2012), the new revolution in vision</font></li><li><font size="+1">DeepFace: Closing the Gap to Human-Level Performance in Face Verification (CVPR, 2014), deep verification is solved.<br></font></li></ol><font style="font-weight: bold;" size="+2">First wave of unsupervised learning (2006-2011)<br></font><ol><li><font size="+1"><a href="http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines">Multiprediction DBM</a> (NIPS, 2013), <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html">CRF as RNN</a> (CVPR, 2015), mean-fields (~1998)</font></li><li><font size="+1"><a href="https://www.ncbi.nlm.nih.gov/pubmed/16873662">Reducing the Dimensionality of Data with Neural Networks</a> (Science, 2006), the new revolution of deep learning<br></font></li></ol><font style="font-weight: bold;" size="+2">Other reading groups<br></font><ol><li><font size="+1"><a href="http://realai.org/">RealAI</a></font></li></ol></body></html>