<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Truyen Tran</title>




















































  

  
  
  <meta content="en-us" http-equiv="Content-Language">

  
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

  
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">

  
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">

  
  <style>
body {
font-family: 'Abel';
font-size: 100px;
}
  </style></head>
<body>
<table id="1" style="border-collapse: collapse; width: 1279px;" border="0" bordercolor="#111111" cellpadding="0" cellspacing="0">

  <tbody>
    <tr>
      <td style="border-right: 1px solid;" v="" bgcolor="#000000">&nbsp;</td>
      <td style="width: 200px;">&nbsp;</td>
      <td style="vertical-align: top;" v="">&nbsp;</td>
    </tr>
    <tr>
      <td rowspan="2" v="" style="border-width: 1px; border-right: 1px solid; vertical-align: top;">
      <p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="generated digits" src="http://rdn-consulting.com/blog/wp-content/uploads/2015/12/deepLearningAI500.png" hspace="0"><br>
      </p>
      <p align="right">[Source:&nbsp;rdn-consulting]</p>
      
      <p align="right">
      </p>
      <p align="right"><font size="5"><a href="index.html">Home</a></font>&nbsp;
&nbsp; </p>
      <br>
      
      <p align="justify">&nbsp;</p>
      
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      </td>
    </tr>
    <tr>
      <td style="width: 24px;">
      
      <p>&nbsp;</p>
      <p>&nbsp;</p>
      </td>
      <td style="vertical-align: top; width: 80%;">
      <h1>Reading Club on Differentiable AI</h1>
      <big>(Thursday, 2-4PM, Room: ka5.231 Waurn Ponds)</big><br>
      <br>
      <ul><li><big>--/--/----: Hardware for AI (TBA)<br>
          </big></li><ul><li><big>Neuromorphic computing, memristor, spintronics</big></li><li><big><a href="https://physicsworld.com/a/spin-glass-provides-insight-into-brain-activity/">Spin-glass solutions</a> (TBA)<br>
          </big></li><li><big><a href="http://science.sciencemag.org/content/361/6406/1004.abstract">All-optical machine learning</a> (TBA)<br>
          </big></li></ul><li><big>--/--/----: Probabilistic programming (TBA)</big></li><li><big>--/--/----: Artificial General Intelligence 2 (TBA)</big></li><li><big>--/--/----: Automated legal reasoning (TBA)<br>
          </big></li>
<li><big>--/--/----: Meta-reasoning, self-awareness and consciousness (Truyen Tran)<br>
          </big></li><li><big>--/--/----: AI across scales (Truyen Tran)<br>
          </big></li><li><big>--/--/----: ICML/NeurIPS test-of-time awards (Truyen Tran)<br>
          </big></li><li><big>--/--/----: Learning world models (TBA)<br>
          </big></li><li><big>--/--/----: Modularity of mind (TBA)<br>
          </big></li>
        <li><big>--/--/----: Digital companion (TBA)<br>
          </big></li>
<li><big>--/--/----: Cognitive architectures (Truyen Tran)<br>
          </big></li>
        <li><big>--/--/----: Free-energy principle (Truyen Tran)</big></li><li><big>--/--/----</big><big>: Competency-based ML (TBA)</big></li><li><big>--/--/----</big><big>: Learning to reason 2</big><big> (Truyen Tran)</big></li><li><big>--/--/----</big><big>: Visual dialog systems</big><big> (TBA)</big></li><li><big>--/--/----</big><big>: One big net for everything 2</big><big> (TBA)</big></li>
        
        <li><big>--/--/----</big><big>: Graph reasoning</big><big> (TBA)</big></li><li><big>--/--/----</big><big>: Sequential learning</big><big> (TBA)</big></li><li><big>--/--/----</big><big>: Fusing RL and PGM</big><big> (TBA)</big></li><li><big>--/--/----</big><big>: </big><big>The work of David Mumford (TBA)</big></li><li><big>--/--/----: AI for scientific discovery (Truyen Tran)<br>
          </big></li><li><big>--/--/----</big><big>: Scientific text mining</big><big> (Truyen Tran)<br></big></li><li><big>--/--/----</big><big>: Multimodal learning and reasoning (TBA)</big></li>
        <li><big>--/--/----</big><big>: NIPS'19 &amp; ICLR'20: Reviews review (TBA)</big></li>
<li><big>07/11/2019</big><big>:&nbsp;</big><big>Value-aligned ML 2 (Truyen Tran)</big></li><li style="color: black;"><big>31/10/2019</big><big>: </big><big>Conversational AI 2 (Van-Khanh Tran)</big></li>
        <li style="color: black;"><big>24/10/2019</big><big>: Moral AI</big><big> 1 (Dung Nguyen)</big></li>
        <li style="font-weight: bold; color: rgb(153, 0, 0);"><big>17/10/2019</big><big>: Visual reasoning</big><big> (Thao Le-Minh)</big></li>
<li style="color: rgb(102, 102, 102);"><big>10/10/2019</big><big>: Hypernets and variational hyper RNN</big><big> (Phuoc Nguyen)</big></li><li style="color: rgb(102, 102, 102);"><big>03/10/2019</big><big>: </big><big>Actions prediction (Romero Morais)&nbsp;</big></li><li style="color: rgb(102, 102, 102);"><big>26/09/2019</big><big>: </big><big>Clinical QA (Tin Pham)</big></li><li style="color: rgb(102, 102, 102);"><big>19/09/2019</big><big>: Latest advances in Neural Turing Machines (Hung Le)</big></li><li style="color: rgb(102, 102, 102);"><big>12/09/2019</big><big>: </big><big><a href="https://arxiv.org/abs/1905.11009">Geometric inference</a> (Cat Le)<br></big></li><li style="color: rgb(102, 102, 102);"><big>29/08/2019</big><big>: </big><big>AAAI'20 works</big> <font size="+1">(Cat: Prior for GP, &nbsp;Dung: Theory of mind, Thao: Relational reasoning, Phuoc: HyperVAE)</font></li><li style="color: rgb(102, 102, 102);"><big>22/08/2019</big><big>: Empirical AI research 3 (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>15/08/2019</big><big>: </big><big>Human priors in video games (Thommen George)</big></li><li style="color: rgb(102, 102, 102);"><big>08/08/2019</big><big>: Spatio-temporal generative models (Xuan-Duc Nguyen)</big></li><li style="color: rgb(102, 102, 102);"><big>01/08/2019</big><big>: </big><big>Disentanglement for video 2 (Kien Do)</big></li><li style="color: rgb(102, 102, 102);"><big>25/07/2019</big><big>: </big><big>Disentanglement for video 1 (Kien Do)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>18/07/2019</big><big>: </big><big>CVPR'19 notes (Vuong Le)</big></li>
<li style="color: rgb(102, 102, 102);"><big>11/07/2019</big><big>: Theoretical DL series 1</big><big> (Hoang Thanh-Tung)</big></li>
<li style="color: rgb(102, 102, 102);"><big>04/07/2019</big><big>: Value-aligned ML 1 (Khanh Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>27/06/2019</big><big>: Empirical AI research 2 (Truyen Tran)</big></li>
<li style="color: rgb(102, 102, 102);"><big>20/06/2019</big><big>: </big><big>Memory-supported RL (Dat Nguyen)</big></li><li style="color: rgb(102, 102, 102);"><big>13/06/2019</big><big>: </big><big>Theory of mind 2 (Dung Nguyen)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>06/06/2019</big><big>: </big><big>Video QA (Thao Le-Minh)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>30/05/2019</big><big>: Empirical AI research 1 (Truyen Tran)</big></li>
<li style="color: rgb(102, 102, 102);"><big>16/05/2019</big><big>: </big><big>VAE for exploration (Phuoc Nguyen)</big><big>; NIPS19 works.</big></li><li style="color: rgb(102, 102, 102);"><big>09/05/2019</big><big>: </big><big>Models for human behaviours in video (Romero Morais)</big></li><li style="color: rgb(102, 102, 102);"><big>02/05/2019</big><big>: </big><big>Neural-symbolic approach to QA 2 (Tin Pham)</big></li><li style="color: rgb(102, 102, 102);"><big>26/04/2019</big><big>: </big><big>Neural-symbolic approach to QA 1 (Tin Pham)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>17/04/2019</big><big>: </big><big>Conversational AI 1 (Van-Khanh Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>11/04/2019</big><big>: </big><big>Disentanglement 2 (Kien Do)</big></li><li style="color: rgb(102, 102, 102);"><big>05/04/2019</big><big>: </big><big>ODE neural nets 2 (Hoang Thanh-Tung)</big></li><li style="color: rgb(102, 102, 102);"><big>28/03/2019</big><big>: </big><big>Neuron, RNN &amp; memory (Hung Le)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>21/03/2019</big><big>: Learning to reason 1 (</big><big>Truyen Tran</big><big>)</big></li><li style="color: rgb(102, 102, 102);"><big>14/03/2019</big><big>: ICCV work: Video QA (Thao Le-Minh); Deep learning in smart-homes (Xuan-Duc Nguyen)</big></li><li style="color: rgb(102, 102, 102);"><big>08/</big><big>03/2019</big><big>: Machine learning in materials science (Duong-Nguyen Nguyen, JAIST Japan)</big></li><li style="color: rgb(102, 102, 102);"><big>01/03/2019</big><big>: <a href="https://truyentran.github.io/talks/AI4matters-2019.pdf">AI for matters</a> (Truyen Tran)</big></li><li style="color: black; font-weight: bold;"><big><span style="font-weight: normal; color: rgb(102, 102, 102);">22/02/2019: Memory-augmented RL 2 (Dat Nguyen)</span><br>
          </big></li>
        <li style="color: rgb(102, 102, 102);"><big>15/02/2019</big><big>: <a href="https://arxiv.org/abs/1802.07740">Machine theory of mind</a></big><big> (Dung Nguyen)<br>
          </big></li>

<li style="color: rgb(102, 102, 102);"><big>11/01/2019</big><big>: Continual learning in GAN &amp;&nbsp;</big><big><a href="https://arxiv.org/abs/1802.08864">One big net for everything</a> </big><big> (</big><big>Hoang Thanh-Tung</big><big>)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>21/12/2018</big><big>: AI/ML in 2019 and beyond: </big><big>Planning </big><big>session 2 (Truyen Tran)<br>
          </big></li>
<li style="color: rgb(102, 102, 102);"><big>14/12/2018</big><big>: AI/ML in 2019 and beyond: </big><big>Planning session </big><big>1 (Truyen Tran)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>30/11/2018: Video QA (Thao Le-Minh)<br>
          </big></li>
        <li style="color: rgb(102, 102, 102);"><big>23/11/2018</big><big>: Communication in Multiagent RL (Dung Nguyen)<br>
          </big></li>
<li style="color: rgb(102, 102, 102);"><big>16/11/2018</big><big>: AI/ML in 2019 and beyond: </big><big>Outlook&nbsp;</big><big>(Truyen Tran)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>09/11/2018:&nbsp;CVPR work: Skeleton trajectories (Romero Barata); </big><big>AI/ML in 2018:&nbsp; Topics &amp; reviewing&nbsp; (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>02/11/2018: DL for </big><big>source code modeling (Huong Ha)</big></li><li style="color: rgb(102, 102, 102);"><big>26/10/2018: Memory-augmented RL 1 (Dat Nguyen)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>19/10/2018: Visual reasoning 2: modularity (Tin Pham)<br>
          </big></li>
        <li style="color: rgb(102, 102, 102);"><big>12/10/2018: Disentanglement via VAE (Kien Do)<br>
          </big></li>
<li style="color: rgb(102, 102, 102);"><big>05/10/2018: Spatio-temporal scene graphs (Vuong Le)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>21/09/2018: ICLR works: MANN writing (Hung Le), GAN convergence (Hoang Thanh-Tung) &amp; graph morphism+RL (Kien Do)<br>
          </big></li>
        <li style="color: rgb(102, 102, 102);"><big>14/09/2018: Graphs apps: Genomics (Thin Nguyen), social media (Hung Nguyen)<br>
          </big></li>
<li style="color: rgb(102, 102, 102);"><big>07/09/2018: Advances in Neural Turing Machines (Truyen Tran)</big><span style="color: rgb(0, 0, 0); font-family: arial,helvetica,clean,sans-serif; font-size: 13.4792px; font-style: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgb(255, 255, 255); display: inline ! important; float: none;"></span></li><li style="color: rgb(102, 102, 102);"><big>31/08/2018: Universal transformer&nbsp; (Adham Beykikhoshk)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>24/08/2018: Learning to count&nbsp; (Tin Pham)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>17/08/2018: Email attachment recommendation (Binh Nguyen)<br>
          </big></li>
        
        <li style="color: rgb(102, 102, 102);"><big>10/08/2018</big><big>: </big><big>Variational continual learning</big><big> (Hoang Thanh-Tung)</big></li>
<li style="color: rgb(102, 102, 102);"><big>03/08/2018</big><big>: </big><big>RNN, memory &amp; attention (cont.) (Hung Le)</big></li>
<li style="color: rgb(102, 102, 102);"><big>27/07/2018</big><big>:</big><big> </big><big>Meta-learning 2 (Truyen Tran)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>20/07/2018</big><big>:</big><big> <a href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017"></a></big><big>Deep reinforcement learning</big><big> (Phuoc Nguyen</big><big>)</big></li><li style="color: rgb(102, 102, 102);"><big>13/07/2018</big><big>:</big><big> </big><big>Variational autoencoders (</big><big>Budhaditya Saha</big><big>)</big></li><li style="color: rgb(102, 102, 102);"><big>06/07/2018</big><big>:</big><big> </big><big>Video prediction models (</big><big>Romero Morais</big><big>)</big></li><li style="color: rgb(102, 102, 102);"><big>29/06/2018</big><big>:</big><big> </big><big>Graph neural nets + RL (Kien Do)</big></li><li style="color: rgb(102, 102, 102);"><big>22/06/2018</big><big>: </big><big>Relational reasoning (Tin Pham)</big></li><li style="color: rgb(102, 102, 102);"><big>15/06/2018</big><big>: </big><big>RNN, memory &amp; attention (Hung Le)</big></li><li style="color: rgb(102, 102, 102);"><big>08/06/2018</big><big>: </big><big>Deep generative models (Hoang Thanh-Tung)</big></li><li style="color: rgb(102, 102, 102);"><big>01/06/2018</big><big>: Deep learning for health &amp; drug design (Truyen Tran)</big></li>
<li style="color: rgb(102, 102, 102);"><big>18/05/2018</big><big>: Scene graphs (Vuong Le)</big></li>
<li style="color: rgb(102, 102, 102);"><big>11/05/2018</big><big>: AI for science 1 (Truyen Tran)</big></li>
        
        <li style="color: rgb(102, 102, 102);"><big>04/05/2018</big><big>: Variational encoder-decoders (Hung Le)</big></li>
<li style="color: rgb(102, 102, 102);"><big>27/04/2018: Visual reasoning (Romero Barata)<br>
          </big></li><li style="color: rgb(102, 102, 102);"><big>20/04/2018: Machine reasoning 1 (Kien Do)<br>
          </big></li>
<li style="color: rgb(102, 102, 102);"><big>13/04/2018</big><big>: Artificial General Intelligence (AGI) 1 </big><big> (Truyen Tran)</big></li><li style="color: rgb(102, 102, 102);"><big>06/04/2018</big><big>: GAN 2 (Hoang Thanh-Tung)</big></li>
        
        <li style="color: rgb(102, 102, 102);"><big>23/03/2018: Graph generative models 2 (Kien Do)<br>
          </big></li>
<li style="color: rgb(102, 102, 102);"><big>16/03/2018: Graph generative models 1 (Kien Do)<br>
          </big></li>
        <li style="color: rgb(102, 102, 102);"><big>09/03/2018</big><big>: The
future of deep learning 4: near/medium-term (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>02/03/2018</big><big>:
The future of deep learning 3: near/medium-term (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>23/02/2018</big><big>:
The future of deep learning 2: near/medium-term (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>09/02/2018: Graph
modeling 2: Molecular models (Trang Pham)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>02/02/2018: RNN
(Hung Le)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>19/01/2018: The
future of deep learning 1: long-term (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>01/12/2017: Deep
learning for biomedicine 1 (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>24/11/2017:
Bayesian deep learning (Hoang Thanh-Tung)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>10/11/2017: Action
recognition on video (Vuong Le)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>03/11/2017:
Capsules (Trang Pham)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>27/10/2017: AAAI'18
reviews review (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>20/10/2017: Text +
knowledge graphs for question answering (Kien Do)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>13/10/2017:
Learning intuitive physics from video (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>29/09/2017: Object
detection (Budhaditya Saha)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>22/09/2017: MANN
2&nbsp; (Hung Le)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>15/09/2017:&nbsp;GAN
1&nbsp; (Hoang Thanh-Tung)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>08/09/2017:<a href="https://www.ijcai.org/proceedings/2017/"> IJCAI-17</a> key
papers (Kien Do, Trang Pham &amp; Phuoc Nguyen)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>01/09/2017:<a href="https://2017.icml.cc/Conferences/2017"> ICML-17</a> review &amp;
key papers (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>04/08/2017:
Meta-learning 1 (Truyen Tran)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>28/07/2017:
Attention mechanisms (Trang Pham)</big></li>
        <li style="color: rgb(102, 102, 102);"><big>21/07/2017: Graph
modeling 1&nbsp; (Kien Do)</big></li>
        <li><big><big><small><span style="color: rgb(102, 102, 102);">14/07/2017:
Memory-augmented neural nets (MANN) 1 (Phuoc Nguyen)</span></small><br>
          </big></big></li>
      </ul>
      <br>
      <font style="font-weight: bold;" size="+2">Capsules<br>
      </font>
      <ol>
        <li><font size="+1"><a href="https://arxiv.org/abs/1710.09829">Dynamic
Routing Between Capsules</a> (Hinton's group, NIPS'17)</font></li>
        <li><font size="+1"><a href="https://openreview.net/forum?id=HJWLfGWRb">Matrix capsules with
EM routing</a> (Hinton's group, ICLR'18)</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Recurrent nets<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://arxiv.org/abs/1609.01704">Learning
nested hierarchy in RNN</a> (Bengio's group, 2016), learn the temporal
hierarchy itself</font></li>
        <li><font size="+1"><a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>
(2016), learn to generate weight matrices, meta-learning, fast synaptic
memory (Hinton), or learn network to learn networks</font></li>
        <li><font size="+1"><a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf">Higherarchical
attention model for document</a>s (NAACL, 2016), attention model</font></li>
        <li><font size="+1"><a href="https://arxiv.org/abs/1603.08983">Adaptive
time computation</a> (2016), learn the determine the length of
computation</font></li>
        <li><font size="+1"><a href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Highway
networks</a> (NIPS, 2015), open the gate</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Optimization<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://arxiv.org/abs/1502.03167">Batch
normalization</a> (ICLR, 2015), a technique for faster learning via a
better conditioning for optimization through normalizing data</font></li>
        <li><font size="+1"><a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">Dropout</a>
(JMLR, 2014), most important discovery in years</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Multi-agent systems<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://arxiv.org/abs/1605.07736">Learning
communication in multiagent</a> (NIPS, 2016), learn to share states
between agents</font></li>
      </ol>
      <br>
      <font style="font-weight: bold;" size="+2">Intuitive Physics<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://papers.nips.cc/paper/6417-interaction-networks-for-learning-about-objects-relations-and-physics">Interaction
networks for learning about objects, relations and physics</a> (NIPS,
2016), simple model for object interactions over time. It's <a href="https://arxiv.org/abs/1706.01433">end-to-end version</a> on
videos (2017).</font></li>
        <li><font size="+1"><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/div-classtitlebuilding-machines-that-learn-and-think-like-peoplediv/A9535B1D745A0377E16C590E14B94993">Building
machines that learn and think like people</a> (Lake et al, 2016),
arguing for cognitively informed machine learning.</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Memory<br>
      </font>
      <ol>
        <li><font size="+1"><a href="https://arxiv.org/pdf/1610.06258v2.pdf">Using Fast Weights to
Attend to the Recent Past</a> (Ba &amp; Hinton, 2016), Hebbian learning
with fast synapse plasticity.</font> <font size="+1">Related to <a href="https://arxiv.org/abs/1609.09106">Hypernetworks</a>. </font></li>
        <li><font size="+1"><a href="http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature20101.pdf">Hybrid
computing using a neural network with dynamic external memory</a>
(DeepMind, 2016), role of external memory. Early version: </font><font size="+1"><a href="http://arxiv.org/abs/1410.5401">Neural Turing
machine</a> (2014).</font></li>
        <li><font size="+1"><a href="https://arxiv.org/abs/1506.07285">Ask
me anything: dynamic memory networks </a>(ICML, 2016), a cool way to
do question answering using memory</font></li>
        <li><font size="+1"><a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks">End-to-end
memory networks</a> (NIPS, 2015), role of memory</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Continual learning<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://arxiv.org/abs/1606.04671">Progressive
neural networks</a> (2016), <a href="http://dl.acm.org/citation.cfm?id=1553380">curriculumn learning</a>
(ICML, 2009),<a href="http://www.sciencedirect.com/science/article/pii/0010027793900584">
learning to start "small"</a> (1992)</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Meta-learning<br>
      </font>
      <ol>
        <li><font size="+1"><a href="https://arxiv.org/abs/1605.06065">Meta-Learning
with Memory-Augmented Neural Networks</a> (ICML, 2016)</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">GAN/VAE/Implicit models<br>
      </font>
      <ol>
        <li><font size="+1"><a href="https://arxiv.org/abs/1701.07875">Wasserstein
GAN</a> (ICML, 2017), a key development of GAN. It's <a href="https://arxiv.org/abs/1704.00028">improved version</a> (ICML,
2017).</font></li>
        <li><font size="+1"><a href="http://papers.nips.cc/paper/5423-generative-adversarial">Generative
Adversarial Nets</a>&nbsp;(NIPS, 2014), a totally new way of generating
high quality data</font></li>
        <li><font size="+1"><a href="http://arxiv.org/abs/1312.6114">Variational
Auto-Encoders</a> (NIPS, 2014), a new powerful way to learn a
generative model</font></li>
        <li><font size="+1"><a href="http://dustintran.com/blog/my-qualifying-exam-oral">A list
compiled by Dustin Tran</a> (2017)<br>
          </font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Graph modeling<br>
      </font>
      <ol>
        <li><font size="+1">Convolutional graphs (multiple papers:
this, this, this and this; 2015, 2016), this is what column net
(version 2016) is about</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Reinforcement learning<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">AlphaGo</a>
(Nature, 2016), the breakthrough of the decade. <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">AlphaGo
Zero</a> (Nature, 2017), the player that learns from absolute zeros.</font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">AI for physical
sciences &amp; biomedicine<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html">DeepBind</a>
(Nature BioTech, 2015), motifs with CNN</font></li>
        <li><font size="+1"><a href="https://truyentran.github.io/acml17-tute.html">Truyen's ACML'17
tutorial</a><br>
          </font></li>
      </ol>
      <br>
      <font style="font-weight: bold;" size="+2">AI for NLP<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://arxiv.org/abs/1609.08144">Google
Neural Machine Translation</a> (2016), <a href="http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces">seq2seq</a>
(NIPS, 2014). Beginning and an end of Neural Machine Translation.</font></li>
        <li><font size="+1"><a href="http://arxiv.org/abs/1507.07998">Paragraph2vec</a>
(ICML, 2014), context ID embedding</font></li>
        <li><font size="+1"><a href="http://arxiv.org/abs/1502.01710">CNNs
for chars in text</a> (2015), end-to-end text classification at
character level<br>
          </font></li>
        <li><font size="+1"><a href="https://arxiv.org/abs/1103.0398">Natural
Language Processing (Almost) from Scratch</a> (JMLR, 2011), CNN for
text for the first time<br>
          </font></li>
        <li><font size="+1"><a href="http://www.jmlr.org/papers/v3/bengio03a.html">Neural nets for
language model</a> (JMLR, 2003), the first of its kind</font></li>
      </ol>
      <br>
      <font style="font-weight: bold;" size="+2">AI for Vision<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-w">ImageNet
Classification with Deep Convolutional Neural Networks</a> (NIPS,
2012), the new revolution in vision</font></li>
        <li><font size="+1">DeepFace: Closing the Gap to Human-Level
Performance in Face Verification (CVPR, 2014), deep verification is
solved.<br>
          </font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">First wave of
unsupervised learning (2006-2011)<br>
      </font>
      <ol>
        <li><font size="+1"><a href="http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines">Multiprediction
DBM</a> (NIPS, 2013), <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html">CRF
as RNN</a> (CVPR, 2015), mean-fields (~1998)</font></li>
        <li><font size="+1"><a href="https://www.ncbi.nlm.nih.gov/pubmed/16873662">Reducing the
Dimensionality of Data with Neural Networks</a> (Science, 2006), the
new revolution of deep learning<br>
          </font></li>
      </ol>
      <font style="font-weight: bold;" size="+2">Other reading groups<br>
      </font>
      <ol>
        <li><font size="+1"><a href="https://casmls.github.io/schedule/">Columbia</a><br>
          </font></li>
      </ol>
      </td>
    </tr>
  </tbody>
</table>

<br>

<br>
</body></html>