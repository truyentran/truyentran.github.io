<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
  <title>Truyen Tran</title>

  
  
  <meta content="en-us" http-equiv="Content-Language">

  
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

  
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">

  
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">

  
  <style type="text/css">
/* Layout-provided Styles */
ul.itemize {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;
}
  </style>
  
  <style>
body {
font-family: 'Abel', serif;
font-size: 12px;
}
  </style>
  
  <meta name="GENERATOR" content="LyX 2.3.1-1">

  
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">

  
  <style type="text/css">
/* Layout-provided Styles */
div.standard {
	margin-bottom: 2ex;
}


  </style>
</head><body>
<table style="border-collapse: collapse; width: 1038px; height: 1304px;" id="1" border="0" bordercolor="#111111" cellpadding="0" cellspacing="0">

  <tbody>
    <tr>
      <td style="border-right: 1px solid;" v="" bgcolor="#000000">&nbsp;</td>
      <td>&nbsp;</td>
      <td style="vertical-align: top;" v="">&nbsp;</td>
    </tr>
    <tr>
      <td rowspan="2" v="" style="border-width: 1px; border-right: 1px solid; vertical-align: top;">
      <p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="AI for drug discovery" src="figs/machine-reasoning.png" hspace="0"><br>
(Source: Venomous Vector)</p>
      <p align="right"> </p>
      <p align="right"> </p>
      <p align="right"> </p>
      <p align="right"><font size="5"><a href="index.html">Home</a></font>&nbsp;
&nbsp; </p>
      <br>
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      </td>
    </tr>
    <tr>
      <td width="24">
      <p>&nbsp;</p>
      <p>&nbsp;</p>
      </td>
      <td width="837">
      <p>
      <table style="text-align: left; margin-left: auto; margin-right: auto; width: 900px; height: 226px;" border="0" bordercolor="#000000" cellpadding="15" cellspacing="3">
        <tbody>
          <tr>
            <td style="background-color: white;"><font style="font-weight: bold; color: rgb(0, 102, 0);" size="+3">From Deep
Learning to Deep Reasoning<br>
            </font><font style="color: rgb(0, 102, 0);" size="+2">A tutorial @KDD 2021, August
14th (Virtual).<br>
            <span style="color: black;"><br>
TL;DR: This tutorial reviews recent developments to extend the capacity
of neural networks to “learning to reason” from data, where the task is
to determine if the data entails a conclusion.</span><br>
            <br>
            </font>
            <table style="text-align: left; width: 874px; height: 631px;" border="0" cellpadding="2" cellspacing="40">
              <tbody>
                <tr>
                  <td style="vertical-align: top; text-align: justify;">
                  
                  
                  <p style="color: black; font-weight: bold;"><font size="+2">Slides
(<span style="text-decoration: underline;">TBA</span>) | Videos
(<span style="text-decoration: underline;">TBA)</span></font></p>
                  <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;">The
rise of big data and big compute has brought modern neural networks to
many walks of digital life, thanks to the relative ease of construction
of large models that scale to the real world. Current successes of
Transformers and self-supervised pretraining on massive data have led
some to believe that deep neural networks will be able to do almost
everything whenever we have data and computational resources. However,
this might not be the case. While neural networks are fast to exploit
surface statistics, they fail miserably to generalize to novel
combinations. Current neural networks do not perform deliberate
reasoning – the capacity to deliberately deduce new knowledge out of
the contextualized data. This tutorial reviews recent developments to
extend the capacity of neural networks to “learning to reason” from
data, where the task is to determine if the data entails a conclusion.
This capacity opens up new ways to generate insights from data through
arbitrary querying using natural languages without the need of
predefining a narrow set of tasks.</span></font></p>

                  </td>
                  <td style="vertical-align: middle; text-align: center;">
                  
                  
                  <p style="color: black; font-weight: bold;"><font size="+2"><span style="text-decoration: underline;"><img style="width: 200px; height: 250px;" alt="A/Prof Truyen Tran" src="figs/13053_4806553394040_1532430269_n.jpg"><br>
                  </span><span style="color: rgb(0, 51, 0);">Presenter: A/Prof Truyen Tran</span><br style="font-weight: normal; color: rgb(0, 51, 0);">
                  <span style="font-weight: normal; color: rgb(0, 51, 0);">Applied AI Institute, Deakin University</span></font></p>
                  </td>
                </tr>
              </tbody>
            </table>
            <br>
            <br>
<p style="color: black; font-weight: bold;"> </p>
            
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Prerequisite</span>:
            <span style="font-style: italic;">the
tutorial assumes some familarity with deep learning.</span></span></font></p>
            
            <p style="color: black; font-weight: bold;"><font size="+2">Content</font></p>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;">The
tutorial consists of three main parts. Part A covers the
learning-to-reason framework, explains how neural networks can serve as
a strong backbone for reasoning through its natural operations such as
binding, attention &amp; dynamic computational graphs. Part B goes into
more details how neural networks perform reasoning over unstructured
and structured data, and across multiple modalities. Reasoning over
sets, relations, graphs and time will be explained. Part C reviews more
advanced topics including systematic generalization, measuring
abstraction, neural Turing machines, social reasoning with theory of
mind, and learning to reason with limited labels. A special attention
will be paid to neural memories as a fundamental mechanism to support
reasoning over entities, relations, and even neural programs. Whenever
possible, case-studies in text understanding and visual question
answering will be presented.<span style="font-weight: bold;"></span><br>
            </span></font></p>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Existing
related talks</span><br>
            </span></font></p>
            <ul>
              <li>
                <p><font size="+2"><span style="font-weight: normal;"></span></font><font size="+2"><span style="font-weight: normal;"></span></font><font size="+2"><span style="font-weight: normal;">Truyen
Tran, Vuong Le, Hung Le and Thao Le, "</span></font><font size="+2"><span style="font-weight: normal;"><a href="https://neuralreasoning.github.io/">Neural machine reasoning</a>",</span></font><font size="+2"><span style="font-weight: normal;"> Tutorial @IJCAI 2021.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></p>
              </li>
              <li>
                <p><font size="+2"><span style="font-weight: normal;">Truyen
Tran, "<a href="https://truyentran.github.io/ssci2020-tute.html">Deep
learning 1.0 and beyond</a>", Tutorial @IEEE SSCI 2020.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></p>
              </li>
              <li>
                <p><font size="+2"><span style="font-weight: normal;">Truyen
Tran, "<a href="https://www.slideshare.net/truyen/machine-reasoning">Machine
reasoning</a>", Invited talk @Monash University, August 2020.</span></font></p>
              </li>
            </ul>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"> </span>Structure:</font></p>
            <p style="color: black; font-weight: bold;"><font size="+2">Part
A: Learning to reason framework (60 mins)<br>
            </font></p>
            <ul>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as a prediction skill that can be learnt from data.</span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Question
answering as zero-shot learning.</span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Neural
network operations for learning to reason:</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Concept-object
binding.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></li>
                <li><font size="+2"><span style="font-weight: normal;">Attention
&amp; transformers.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></li>
                <li><font size="+2"><span style="font-weight: normal;">Dynamic
neural networks, conditional computation &amp; differentiable
programming.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></li>
              </ul>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as iterative representation refinement.</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Compositional
attention networks.</span></font></li>
              </ul>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as query-driven program synthesis and execution.</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Neural
module networks.<br>
                  </span></font></li>
              </ul>
            </ul>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Part B:
Reasoning over unstructured and structured data (60 mins)</span></span><br style="font-weight: normal;">
            </font></p>
            <ul>
              <li><font size="+2"><span style="font-weight: normal;">Cross-modality
reasoning, the case of vision-language integration.</span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as set-set interaction.</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Query
processing.</span></font></li>
              </ul>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Context
processing.</span></font></li>
                <li><font size="+2"><span style="font-weight: normal;">Dual-attention.</span></font></li>
              </ul>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Conditional
set functions.</span></font>
                  <ul>
                  </ul>
                  <font size="+2"><span style="font-weight: normal;"></span></font></li>
              </ul>
              <li><font size="+2"><span style="font-weight: normal;">Relational
reasoning</span></font>
                <ul>
                  <li><font size="+2"><span style="font-weight: normal;">Relation
networks</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Graph
neural networks</span></font></li>
                </ul>
                <font size="+2"><span style="font-weight: normal;"></span></font>
                <ul style="margin-left: 40px;">
                  <li><font size="+2"><span style="font-weight: normal;">Graph
embedding.</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Graph
convolutional networks.</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Graph
attention.</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Message
passing.</span></font></li>
                </ul>
                <font size="+2"><span style="font-weight: normal;"> </span></font>
                <ul>
                  <li><font size="+2"><span style="font-weight: normal;">Query-conditioned
dynamic graph constructions</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Reasoning
over knowledge graphs.</span></font></li>
                </ul>
                <font size="+2"><span style="font-weight: normal;"> </span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">&nbsp;Temporal
reasoning<br>
                </span></font>
                <ul>
                  <li><font size="+2"><span style="font-weight: normal;">Video
question answering. </span></font></li>
                </ul>
              </li>
            </ul>
            <p style="color: black; font-weight: bold;"><font size="+2">Part
C: Advanced topics (60 mins)<br>
            </font></p>
            <ul>
              <li><font size="+2">Systematic generalization</font></li>
              <li><font size="+2">Measuring reasoning capability.</font></li>
              <li><font size="+2">Reasoning with external memories</font></li>
              <ul>
                <li><font size="+2">Memory of entities –
memory-augmented neural networks</font></li>
                <li><font size="+2">Memory of relations with tensors
and graphs</font></li>
                <li><font size="+2">Memory of programs &amp; neural
program construction.</font></li>
              </ul>
              <li><font size="+2">Learning to reason with less labels:</font></li>
              <ul>
                <li><font size="+2">Data augmentation with analogical
and counterfactual examples</font></li>
                <li><font size="+2">Question generation</font></li>
                <li><font size="+2">Self-supervised learning for
question answering</font></li>
                <li><font size="+2">Learning with external knowledge
graphs</font></li>
              </ul>
              <li><font size="+2">Social reasoning with neural theory
of mind.</font></li>
            </ul>
            </td>
          </tr>
        </tbody>
      </table>
      </p>
      <br>
      <p style="margin-left: 40px;" align="justify"><font style="font-weight: bold;" size="5">References (TBA)</font></p>
      <ol>
        <li>
          <p><font size="5">Jacob Andreas, Marcus Rohrbach, Trevor
Darrell, and Dan Klein. “Neural module networks”. In <span style="font-style: italic;">CVPR</span>, pages 39–48, 2016.</font></p>
        </li>
        <li>
          <p><font size="5">Dzmitry Bahdanau, Shikhar Murty, Michael
Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville.
“Systematic generalization: what is required and can it be learned?”,
In <span style="font-style: italic;">ICLR</span>, 2019.</font></p>
        </li>
        <li>
          <p><font size="5">Bottou, Léon. “From machine learning to
machine reasoning”.&nbsp;<span style="font-style: italic;">Machine
learning</span>&nbsp;94.2 (2014): 133-149. <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Jerry A Fodor and Zenon W Pylyshyn.
“Connectionism and cognitive architecture: A critical analysis”. <span style="font-style: italic;">Cognition</span>, 28(1-2):3–71, 1988.</font></p>
        </li>
        <li>
          <p><font size="5">Marta Garnelo and Murray Shanahan.
“Reconciling deep learning with symbolic artificial intelligence:
representing objects and relations”. <span style="font-style: italic;">Current
Opinion in Behavioral Sciences</span>, 29:17–23, 2019.</font></p>
        </li>
        <li>
          <p><font size="5">Alex Graves, Greg Wayne, Malcolm Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska- Barwin´ska, Sergio Go´mez
Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
“Hybrid computing using a neural network with dynamic external memory”.
          <span style="font-style: italic;">Nature</span>,
538(7626):471–476, 2016.</font></p>
        </li>
        <li>
          <p><font size="5">Klaus Greff, Sjoerd van Steenkiste, and
Jurgen Schmidhuber. “On the binding problem in artificial neural
networks”. <span style="font-style: italic;">arXiv preprint</span>
arXiv:2012.05208, 2020. <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Drew A Hudson and Christopher D Manning.
“Compositional attention networks for machine reasoning”. In <span style="font-style: italic;">ICLR</span>, 2018.</font></p>
        </li>
        <li>
          <p><font size="5">Roni Khardon and Dan Roth. “Learning to
reason”. <span style="font-style: italic;">Journal of the ACM</span>
(JACM), 44(5):697–725, 1997.</font></p>
        </li>
        <li>
          <p><font size="5">Brenden M Lake, Tomer D Ullman, Joshua B
Tenenbaum, and Samuel J Gershman. “Building machines that learn and
think like people”. <span style="font-style: italic;">Behavioral and
Brain Sciences</span>, 40, 2017. <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Lamb, Luis C., Artur Garcez, Marco Gori,
Marcelo Prates, Pedro Avelar, and Moshe Vardi. “Graph Neural Networks
Meet Neural-Symbolic Computing: A Survey and Perspective.” In <span style="font-style: italic;">Proceedings of IJCAI</span> 2020. <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Hung Le, Truyen Tran, and Svetha Venkatesh.
“Neural stored-program memory”. In <span style="font-style: italic;">ICLR</span>,
2020.</font></p>
        </li>
        <li>
          <p><font size="5">Hung Le, Truyen Tran, and Svetha Venkatesh.
“Self-attentive associative memory”. In <span style="font-style: italic;">ICML</span>, 2020.</font></p>
        </li>
        <li>
          <p><font size="5">Thao Minh Le, Vuong Le, Svetha Venkatesh,
and Truyen Tran. “Dynamic language binding in relational visual
reasoning”. In <span style="font-style: italic;">IJCAI</span>, 2020.</font></p>
        </li>
        <li>
          <p><font size="5">Le, Thao Minh, Vuong Le, Svetha Venkatesh,
and Truyen Tran. “Hierarchical conditional relation networks for video
question answering.” In <span style="font-style: italic;">Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
pp. 9972-9981. 2020.</font></p>
        </li>
        <li>
          <p><font size="5">Mao, Jiayuan, et al. “The Neuro-Symbolic
Concept Learner: Interpreting Scenes, Words, and Sentences From Natural
Supervision.”,&nbsp;In I<span style="font-style: italic;">nternational
Conference on Learning Representations</span>. 2019. <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Marcus, Gary. “Deep learning: A critical
appraisal.”&nbsp;<span style="font-style: italic;">arXiv preprint</span>
arXiv:1801.00631&nbsp;(2018).</font></p>
        </li>
        <li>
          <p><font size="5">Rasmus Palm, Ulrich Paquet, and Ole
Winther. “Recurrent relational networks”. In <span style="font-style: italic;">NeurIPS</span>, pages 3368–3378, 2018.</font></p>
        </li>
        <li>
          <p><font size="5">Ethan Perez, Florian Strub, Harm De Vries,
Vincent Dumoulin, and Aaron Courville. “Film: Visual reasoning with a
general conditioning layer”. In <span style="font-style: italic;">AAAI</span>,
2018.</font></p>
        </li>
        <li>
          <p><font size="5">Rabinowitz, Neil C., et al. “Machine theory
of mind.”&nbsp;In <span style="font-style: italic;">ICML</span>&nbsp;(2018).
          <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Ramsauer, Hubert, et al. “Hopfield networks
is all you need.”&nbsp;<span style="font-style: italic;">arXiv preprint</span>
arXiv:2008.02217&nbsp;(2020). <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Adam Santoro, David Raposo, David G
Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim
Lillicrap. “A simple neural network module for relational reasoning”.
In <span style="font-style: italic;">NIPS</span>, pages 4974–4983,
2017.</font></p>
        </li>
        <li>
          <p><font size="5">Imanol Schlag and Jurgen Schmidhuber.
“Learning to reason with third order tensor products”. In <span style="font-style: italic;">Advances in Neural Information Processing
Systems</span>, pages 9981–9993, 2018.</font></p>
        </li>
        <li>
          <p><font size="5">Minjoon Seo, Aniruddha Kembhavi, Ali
Farhadi, and Hannaneh Hajishirzi. “Bidirectional attention flow for
machine comprehension”. In <span style="font-style: italic;">ICLR</span>,
2017.</font></p>
        </li>
        <li>
          <p><font size="5">Sainbayar Sukhbaatar, Arthur Szlam, Jason
Weston, and Rob Fergus. “End-to-end memory networks”. In <span style="font-style: italic;">NIPS</span>, 2015.</font></p>
        </li>
        <li>
          <p><font size="5">Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I.
“Attention is all you need”. In <span style="font-style: italic;">NIPS</span>,
2017.</font></p>
        </li>
        <li>
          <p><font size="5">Veličković, Petar, Guillem Cucurull,
Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. “Graph
attention networks.”, In <span style="font-style: italic;">ICLR</span>,
2018.</font></p>
        </li>
        <li>
          <p><font size="5">Caiming Xiong, Stephen Merity, and Richard
Socher. “Dynamic memory networks for visual and textual question
answering”. In <span style="font-style: italic;">International
Conference on Machine Learning</span>, pages 2397–2406, 2016.</font></p>
        </li>
        <li>
          <p><font size="5">Keyulu Xu, Jingling Li, Mozhi Zhang, Simon
S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. “What can neural
networks reason about?” In <span style="font-style: italic;">ICLR</span>,
2020.</font><font style="font-weight: bold;" size="5"><br>
          </font></p>
        </li>
      </ol>
      <ol style="margin-left: 40px;">
      </ol>
      </td>
    </tr>
  </tbody>
</table>

<p align="justify">&nbsp;</p>

</body></html>