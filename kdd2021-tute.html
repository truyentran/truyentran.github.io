<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Truyen Tran</title>




  

  
  
  <meta content="en-us" http-equiv="Content-Language">

  
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

  
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">

  
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">

  
  <style type="text/css">
/* Layout-provided Styles */
ul.itemize {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;
}
  </style>
  
  <style>
body {
font-family: 'Abel', serif;
font-size: 12px;
}
  </style>
  
  <meta name="GENERATOR" content="LyX 2.3.1-1">

  
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">

  
  <style type="text/css">
/* Layout-provided Styles */
div.standard {
	margin-bottom: 2ex;
}


  </style></head><body>
<table style="border-collapse: collapse; width: 1038px; height: 1304px;" id="1" border="0" bordercolor="#111111" cellpadding="0" cellspacing="0">

  <tbody>
    <tr>
      <td style="border-right: 1px solid;" v="" bgcolor="#000000">&nbsp;</td>
      <td>&nbsp;</td>
      <td style="vertical-align: top;" v="">&nbsp;</td>
    </tr>
    <tr>
      <td rowspan="2" v="" style="border-width: 1px; border-right: 1px solid; vertical-align: top;">
      <p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="AI for drug discovery" src="figs/machine-reasoning.png" hspace="0"><br>
(Source: Venomous Vector)</p>
      <p align="right"> </p>
      <p align="right"> </p>
      <p align="right"> </p>
      <p align="right"><font size="5"><a href="index.html">Home</a></font>&nbsp;
&nbsp; </p>
      <br>
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      </td>
    </tr>
    <tr>
      <td width="24">
      <p>&nbsp;</p>
      <p>&nbsp;</p>
      </td>
      <td width="837">
      <p>
      <table style="text-align: left; margin-left: auto; margin-right: auto; width: 900px; height: 226px;" border="0" bordercolor="#000000" cellpadding="15" cellspacing="3">
        <tbody>
          <tr>
            <td style="background-color: white;"><font style="font-weight: bold; color: rgb(0, 102, 0);" size="+3">From Deep
Learning to Deep Reasoning<br>
            </font><font style="color: rgb(0, 102, 0);" size="+2">A tutorial @KDD 2021, August
14th (Virtual).<br>
            <span style="color: black;"><br>
TL;DR: <span style="font-style: italic;">This tutorial reviews recent
developments to extend the capacity of neural networks to “learning to
reason” from data, where the task is to determine if the data entails a
conclusion</span>.<br>
            <br>
            </span></font><font size="+2"><span style="color: rgb(0, 51, 0);"><span style="font-weight: bold;">Presenters</span> </span></font><br>
            <table style="text-align: left; width: 100%; margin-left: auto; margin-right: auto;" border="0" cellpadding="2" cellspacing="2">
              <tbody>
                <tr>
                  <td style="vertical-align: top; text-align: center;">
                  <p style="color: black; font-weight: bold;"><font size="+2"><span style="text-decoration: underline;"><img style="width: 150px; height: 188px;" alt="A/Prof Truyen Tran" src="figs/13053_4806553394040_1532430269_n.jpg"><br>
                  </span><span style="font-weight: normal; color: rgb(0, 51, 0);">A/Prof Truyen Tran<br>
</span></font></p>
                  </td>
                  <td style="vertical-align: top; width: 25%; text-align: center;">
                  <p style="color: black; font-weight: bold;"><font size="+2"><span style="text-decoration: underline;"><img style="width: 147px; height: 188px;" alt="Dr Vuong Le" src="figs/vuong.jpg"><br>
                  </span><span style="font-weight: normal; color: rgb(0, 51, 0);">Dr Vuong Le<br>
</span></font></p>

                  </td>
                  <td style="vertical-align: top; width: 25%; text-align: center;">
                  <p style="color: black; font-weight: bold;"><font size="+2"><span style="text-decoration: underline;"><img style="width: 144px; height: 188px;" alt="Dr Hung Le" src="figs/hung.jpg"><br>
                  </span><span style="font-weight: normal; color: rgb(0, 51, 0);">Dr Hung Le<br>
</span></font></p>

                  </td>
                  <td style="vertical-align: top; width: 25%; text-align: center;">
                  <p style="color: black; font-weight: bold;"><font size="+2"><span style="text-decoration: underline;"><img style="width: 149px; height: 188px;" alt="Dr Thao Le" src="figs/thao.jpg"><br>
                  </span><span style="font-weight: normal; color: rgb(0, 51, 0);">Dr Thao Le<br>
</span></font></p>

                  </td>
                </tr>
                <tr>
                  <td colspan="4" rowspan="1" style="vertical-align: top; text-align: center;"><font size="+2"><span style="font-weight: normal; color: rgb(0, 51, 0);">Applied AI Institute, Deakin University</span></font></td>
                </tr>
              </tbody>
            </table>
            
            <p style="color: black; font-weight: bold;"><font size="+2">Slides
(<a href="https://truyentran.github.io/talks/Part-A-Learning-To-Reason_v2.pdf">Part A</a> | <a href="https://truyentran.github.io/talks/Part-B-Reasoning-over-structures_v3.pdf">Part B</a> | <a href="https://truyentran.github.io/talks/Part-C-Advanced-topics.pdf">Part C</a>)<span style="text-decoration: underline;"></span></font></p>

            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;">The
rise of big data and big compute has brought modern neural networks to
many walks of digital life, thanks to the relative ease of construction
of large models that scale to the real world. Current successes of
Transformers and self-supervised pretraining on massive data have led
some to believe that deep neural networks will be able to do almost
everything whenever we have data and computational resources. However,
this might not be the case. While neural networks are fast to exploit
surface statistics, they fail miserably to generalize to novel
combinations. Current neural networks do not perform deliberate
reasoning – the capacity to deliberately deduce new knowledge out of
the contextualized data. This tutorial reviews recent developments to
extend the capacity of neural networks to “learning to reason” from
data, where the task is to determine if the data entails a conclusion.
This capacity opens up new ways to generate insights from data through
arbitrary querying using natural languages without the need of
predefining a narrow set of tasks.</span></font></p>

            <font style="color: rgb(0, 102, 0);" size="+2"><span style="color: black;"></span></font><p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Prerequisite</span>:
            <span style="font-style: italic;">the
tutorial assumes some familarity with deep learning.</span></span></font></p>
            
            <p style="color: black; font-weight: bold;"><font size="+2">Content</font></p>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;">The
tutorial consists of three main parts. Part A covers the
learning-to-reason framework, explains how neural networks can serve as
a strong backbone for reasoning through its natural operations such as
binding, attention &amp; dynamic computational graphs. We will also
show how neural networks can learn to perform combinatoric algorithms.
Part B goes into
more details how neural networks perform reasoning over unstructured
and structured data, and across multiple modalities. Reasoning over
sets, relations, graphs and time will be explained. Part C reviews more
advanced topics including neural nets with external memories, </span></font><font size="+2"><span style="font-weight: normal;">learning to reason with limited labels, and</span></font><font size="+2"><span style="font-weight: normal;"> recursive reasoning with theory of
mind. A special attention
will be paid to neural memories as a fundamental mechanism to support
reasoning over entities, relations, and even neural programs. Whenever
possible, case-studies in text understanding and visual question
answering will be presented.<span style="font-weight: bold;"></span><br>
            </span></font></p>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Existing
related talks</span><br>
            </span></font></p>
            <ul>
              <li>
                <p><font size="+2"><span style="font-weight: normal;"></span></font><font size="+2"><span style="font-weight: normal;"></span></font><font size="+2"><span style="font-weight: normal;">Truyen
Tran, Vuong Le, Hung Le and Thao Le, "</span></font><font size="+2"><span style="font-weight: normal;"><a href="https://neuralreasoning.github.io/">Neural machine reasoning</a>",</span></font><font size="+2"><span style="font-weight: normal;"> Tutorial @IJCAI 2021.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></p>
              </li>
              <li>
                <p><font size="+2"><span style="font-weight: normal;">Truyen
Tran, "<a href="https://truyentran.github.io/ssci2020-tute.html">Deep
learning 1.0 and beyond</a>", Tutorial @IEEE SSCI 2020.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></p>
              </li>
              <li>
                <p><font size="+2"><span style="font-weight: normal;">Truyen
Tran, "<a href="https://www.slideshare.net/truyen/machine-reasoning">Machine
reasoning</a>", Invited talk @Monash University, August 2020.</span></font></p>
              </li>
            </ul>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"> </span>Structure:</font></p>
            <p style="color: black; font-weight: bold;"><font size="+2">Part
A: Learning to reason framework (60 mins)<br>
            </font></p>
            <ul>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as a prediction skill that can be learnt from data.</span></font></li>
              
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Question
answering as zero-shot learning.</span></font></li>
              </ul>

              <li><font size="+2"><span style="font-weight: normal;">Neural
network operations for learning to reason:</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Concept-object
binding.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></li>
                <li><font size="+2"><span style="font-weight: normal;">Attention
&amp; transformers.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></li>
                <li><font size="+2"><span style="font-weight: normal;">Dynamic
neural networks, conditional computation &amp; differentiable
programming.</span></font><font size="+2"><span style="font-weight: normal;"></span></font></li>
              </ul>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as iterative representation refinement &amp; </span></font><font size="+2"><span style="font-weight: normal;">query-driven program synthesis and execution.</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Compositional
attention networks.</span></font></li>
              </ul>
              
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Neural
module networks.</span></font></li>
              </ul>
              <li><font size="+2"><span style="font-weight: normal;">Combinatorics reasoning.<br>
                </span></font></li>

            </ul>
            <p style="color: black; font-weight: bold;"><font size="+2"><span style="font-weight: normal;"><span style="font-weight: bold;">Part B:
Reasoning over unstructured and structured data (60 mins)</span></span><br style="font-weight: normal;">
            </font></p>
            <ul>
              <li><font size="+2"><span style="font-weight: normal;">Cross-modality
reasoning, the case of vision-language integration.</span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">Reasoning
as set-set interaction.</span></font></li>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Query
processing.</span></font></li>
              </ul>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Context
processing.</span></font></li>
                <li><font size="+2"><span style="font-weight: normal;">Dual-attention.</span></font></li>
              </ul>
              <ul>
                <li><font size="+2"><span style="font-weight: normal;">Conditional
set functions.</span></font>
                  <ul>
                  </ul>
                  <font size="+2"><span style="font-weight: normal;"></span></font></li>
              </ul>
              <li><font size="+2"><span style="font-weight: normal;">Relational
reasoning</span></font>
                <ul>
                  <li><font size="+2"><span style="font-weight: normal;">Relation
networks</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Graph
neural networks</span></font></li>
                </ul>
                <font size="+2"><span style="font-weight: normal;"></span></font>
                <ul style="margin-left: 40px;">
                  <li><font size="+2"><span style="font-weight: normal;">Graph
embedding.</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Graph
convolutional networks.</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Graph
attention.</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Message
passing.</span></font></li>
                </ul>
                <font size="+2"><span style="font-weight: normal;"> </span></font>
                <ul>
                  <li><font size="+2"><span style="font-weight: normal;">Query-conditioned
dynamic graph constructions</span></font></li>
                  <li><font size="+2"><span style="font-weight: normal;">Reasoning
over knowledge graphs.</span></font></li>
                </ul>
                <font size="+2"><span style="font-weight: normal;"> </span></font></li>
              <li><font size="+2"><span style="font-weight: normal;">&nbsp;Temporal
reasoning<br>
                </span></font>
                <ul>
                  <li><font size="+2"><span style="font-weight: normal;">Video
question answering. </span></font></li>
                </ul>
              </li>
            </ul>
            <p style="color: black; font-weight: bold;"><font size="+2">Part
C: Advanced topics (60 mins)<br>
            </font></p>
            <ul><li><font size="+2">Reasoning with external memories</font></li>
              <ul>
                <li><font size="+2">Memory of entities –
memory-augmented neural networks</font></li>
                <li><font size="+2">Memory of relations with tensors
and graphs</font></li>
                <li><font size="+2">Memory of programs &amp; neural
program construction.</font></li>
              </ul>
              <li><font size="+2">Learning to reason with less labels:</font></li>
              <ul>
                <li><font size="+2">Data augmentation with analogical
and counterfactual examples</font></li>
                <li><font size="+2">Question generation</font></li>
                <li><font size="+2">Self-supervised learning for
question answering</font></li>
                <li><font size="+2">Learning with external knowledge
graphs</font></li>
              </ul>
              <li><font size="+2">Recursive reasoning with neural theory
of mind.</font></li>
            </ul>
            </td>
          </tr>
        </tbody>
      </table>
      </p>
      <br>
      <p style="margin-left: 40px;" align="justify"><font style="font-weight: bold;" size="5">References<br>
</font></p>
      <ol>
        <li>
          <p><font size="5">Abboud,
Ralph, Ismail Ceylan, and Thomas Lukasiewicz. "Learning to reason:
Leveraging neural networks for approximate DNF counting.“ In <span style="font-style: italic;">Proceedings of the AAAI Conference on Artificial Intelligence</span>, vol. 34, no. 04, pp. 3097-3104. 2020.</font></p></li>
        <li>
          <p><font size="5">Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In <span style="font-style: italic;">CVPR</span>, pages 39–48, 2016.</font></p>
        </li>
        <li>
          <p><font size="5">Bahdanau, Dzmitry, Shikhar Murty, Michael
Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville.
Systematic generalization: what is required and can it be learned? <span style="font-style: italic;">ICLR</span>, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Bai, Yunsheng, Derek Xu, Alex Wang, Ken Gu,
Xueqing Wu, Agustin Marinovic, Christopher Ro, Yizhou Sun, and Wei
Wang. Fast detection of maximum common subgraph via deep q-learning. <span style="font-style: italic;">arXiv preprint</span> arXiv:2002.03129, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Barcelo´, Pablo, Egor V Kostylev, Mikael
Monet, Jorge Pe´rez, Juan Reutter, and Juan Pablo Silva. The logical
expressiveness of graph neural networks. In <span style="font-style: italic;">International Conference on Learning Representations</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Battaglia, Peter W, Jessica B Hamrick,
Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz
Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph
networks. <span style="font-style: italic;">arXiv preprint</span> arXiv:1806.01261, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Bottou, Léon. “From machine learning to machine reasoning”. <span style="font-style: italic;">Machine learning</span> 94.2 (2014): 133-149.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Buckner, Cameron and James Garson. Connectionism. In Edward N. Zalta, editor, <span style="font-style: italic;">The Stanford Encyclopedia of Philosophy</span>. Metaphysics Research Lab, Stanford University, fall 2019 edition, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Chen, Zhengdao, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substruc- tures? <span style="font-style: italic;">arXiv preprint</span> arXiv:2002.04025, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Dang, Long Hoang, et al. "Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering."<span style="font-style: italic;"> IJCAI’21</span><br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Dang, Long Hoang, Thao Minh Le, Vuong Le, and Truyen Tran. Object-centric relational reasoning for video question answering, <span style="font-style: italic;">IJCNN 2021</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Dehghani, Mostafa, et al. "Universal Transformers." <span style="font-style: italic;">International Conference on Learning Representations</span>. 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Devlin, Jacob, Ming-Wei Chang, Kenton Lee,
and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In <span style="font-style: italic;">NAACL-HLT</span> (1), 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Eichenbaum, Howard, <span style="font-style: italic;">Memory, amnesia, and the hippocampal system</span> (MIT press, 1993).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Evans, Jonathan St and Keith E Stanovich. Dual-process theories of higher cognition: Advancing the debate. <span style="font-style: italic;">Perspectives on psychological science</span>, 8(3):223–241, 2013.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Fan, Chenyou, et al. "Heterogeneous memory enhanced multimodal attention model for video question answering." <span style="font-style: italic;">CVPR’19</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Feeney, Aidan and Valerie A Thompson. Reasoning as memory. <span style="font-style: italic;">Psychology Press</span>, 2014.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Fodor, Jerry A and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. <span style="font-style: italic;">Cognition</span>, 28(1-2):3–71, 1988.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Gao, Jiyang, et al. "Motion-appearance co-memory networks for video question answering." <span style="font-style: italic;">CVPR’18</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Garcez, Artur d’Avila, Marco Gori, Luis C
Lamb, Luciano Serafini, Michael Spranger, and Son N Tran.
Neural-symbolic computing: An effective methodology for principled
integration of machine learning and reasoning. <span style="font-style: italic;">arXiv preprint </span>arXiv:1905.06088, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Garnelo, Marta and Murray Shanahan.
Reconciling deep learning with symbolic artificial intelligence:
representing objects and relations. <span style="font-style: italic;">Current Opinion in Behavioral Sciences</span>, 29:17–23, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Gasse, Maxime, Didier Che´telat, Nicola
Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial
optimization with graph convolutional neural networks. <span style="font-style: italic;">NeurIPS</span>, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Gokhale, Tejas, et al. "Mutant: A training paradigm for out-of-distribution generalization in visual question answering." <span style="font-style: italic;">EMNLP’20</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Goyal, Anirudh, Alex Lamb, Jordan Hoffmann,
Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Scho¨lkopf.
Recurrent independent mechanisms. <span style="font-style: italic;">arXiv preprint</span> arXiv:1909.10893, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Graves, Alex, Greg Wayne, and Ivo Danihelka. "Neural turing machines." <span style="font-style: italic;">arXiv preprint</span> arXiv:1410.5401 (2014).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Graves, Alex, Greg Wayne, Malcolm Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska- Barwin´ska, Sergio Go´mez
Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. <span style="font-style: italic;">Nature</span>, 538(7626):471–476, 2016.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Greff, Klaus, Sjoerd van Steenkiste, and Jurgen Schmidhuber. “On the binding problem in artificial neural networks”. <span style="font-style: italic;">arXiv preprint</span> arXiv:2012.05208, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Ha, David, Andrew Dai, and Quoc V. Le. "Hypernetworks." <span style="font-style: italic;">arXiv preprint</span> arXiv:1609.09106 (2016).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Heit, Evan, and Brett K. Hayes. "Predicting reasoning from memory." <span style="font-style: italic;">Journal of Experimental Psychology: General</span> 140, no. 1 (2011): 76.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Heskes, Tom. "Stable fixed points of loopy belief propagation are local minima of the bethe free energy." <span style="font-style: italic;">Advances in neural information processing systems</span>. 2003.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Hu, Ronghang, Anna Rohrbach, Trevor Darrell, and Kate Saenko. Language-conditioned graph networks for relational reasoning. <span style="font-style: italic;">ICCV</span>, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Hu, Ronghang, Jacob Andreas, Marcus
Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:
End-to-end module networks for visual question answering. In <span style="font-style: italic;">ICCV</span>, pages 804–813. IEEE, 2017.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Hudson, Drew A and Christopher D Manning. Compositional attention networks for machine reasoning. <span style="font-style: italic;">ICLR</span>, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Kahneman, Daniel. <span style="font-style: italic;">Thinking, fast and slow</span>. Farrar, Straus and Giroux New York, 2011.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Khardon, Roni and Dan Roth. Learning to reason.<span style="font-style: italic;"> Journal of the ACM</span> (JACM), 44(5):697–725, 1997.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Konkel, Alex and Neal J Cohen. Relational memory and the hippocampus: representations and methods. <span style="font-style: italic;">Frontiers in neuroscience</span>, 3:23, 2009.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Krishna, Ranjay, Michael Bernstein, and Li Fei-Fei. "Information maximizing visual question generation." <span style="font-style: italic;">CVPR</span>’19.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Kuleshov, Volodymyr and Stefano Ermon. Neural variational inference and learning in undirected graphical models. In <span style="font-style: italic;">Advances in Neural Information Processing Systems</span>, pages 6734–6743, 2017.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Lake, Brenden M, Tomer D Ullman, Joshua B
Tenenbaum, and Samuel J Gershman. “Building machines that learn and
think like people”. <span style="font-style: italic;">Behavioral and Brain Sciences</span>, 40, 2017.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Lamb, Luis C., Artur Garcez, Marco Gori,
Marcelo Prates, Pedro Avelar, and Moshe Vardi. “Graph Neural Networks
Meet Neural-Symbolic Computing: A Survey and Perspective.” In <span style="font-style: italic;">Proceedings of IJCAI 2020</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Hung, , Truyen Tran, and Svetha Venkatesh. Learning to remember more with less memorization. In<span style="font-style: italic;"> ICLR</span>’19, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Hung, Truyen Tran, and Svetha Venkatesh. Neural stored-program memory. In <span style="font-style: italic;">ICLR</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Hung, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory. In<span style="font-style: italic;"> ICML</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Thao Minh, Vuong Le, Svetha Venkatesh, and Truyen Tran. Dynamic language binding in relational visual reasoning. In <span style="font-style: italic;">IJCAI</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Thao Minh, Vuong Le, Svetha Venkatesh,
and Truyen Tran. Hierarchical conditional relation networks for
multimodal video question answering. <span style="font-style: italic;">International Journal of Computer Vision</span>, 2021.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Thao Minh, Vuong Le, Svetha Venkatesh,
and Truyen Tran. Hierarchical conditional relation networks for video
question answering. In <span style="font-style: italic;">CVPR</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Le, Thao Minh, Vuong Le, Svetha Venkatesh, and Truyen Tran. Neural reasoning, fast and slow, for video question answering. In <span style="font-style: italic;">IJCNN</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Lei, Jie, et al. "Less is more: Clipbert for video-and-language learning via sparse sampling." <span style="font-style: italic;">CVPR</span>’21.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Lei, Jie, et al. "Tvqa: Localized, compositional video question answering." <span style="font-style: italic;">EMNLP’18</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Lemos, Henrique, Marcelo Prates, Pedro Avelar, and Luis <br>
Lamb. Graph colouring meets deep learning: Effective graph neural network models for combinatorial problems. <span style="font-style: italic;">arXiv preprint</span> arXiv:1903.04598, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Li, Linjie, et al. "Hero: Hierarchical encoder for video+ language omni-representation pre-training." <span style="font-style: italic;">EMNLP’20</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Liu, Yongfei, Bo Wan, Xiaodan Zhu, and Xuming He. Learning cross-modal context graph for visual grounding. <span style="font-style: italic;">AAAI</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Ma, Qiang, Suwen Ge, Danyang He, Darshan
Thaker, and Iddo Drori. Combinatorial optimization by graph pointer
networks and hierarchical reinforcement learning. <span style="font-style: italic;">arXiv preprint </span>arXiv:1911.04936, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Mao, Jiayuan, et al. “The Neuro-Symbolic
Concept Learner: Interpreting Scenes, Words, and Sentences From Natural
Supervision.”, In <span style="font-style: italic;">International Conference on Learning Representations</span>. 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Marcus, Gary. “Deep learning: A critical appraisal.” <span style="font-style: italic;">arXiv preprint </span>arXiv:1801.00631 (2018).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Marino, Kenneth, et al. "Ok-vqa: A visual question answering benchmark requiring external knowledge." <span style="font-style: italic;">CVPR’19</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Morais, Romero, Vuong Le, Truyen Tran, and Svetha Venkatesh. Learning to abstract and predict human actions. In <span style="font-style: italic;">British Machine Vision Conference (BMVC)</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Narasimhan, M., Lazebnik, S., &amp;
Schwing, A. G. (2018). Out of the box: Reasoning with graph convolution
nets for factual visual question answering. <span style="font-style: italic;">Advances in Neural Information Processing Systems</span>, 2018, 2654-2665.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Nguyen, Dung, et al. "Theory of Mind with Guilt Aversion Facilitates Cooperative Reinforcement Learning." <span style="font-style: italic;">Asian Conference on Machine Learning</span>. PMLR, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Palm, Rasmus Berg, Ulrich Paquet, and Ole Winther. "Recurrent Relational Networks." In <span style="font-style: italic;">NeurIPS</span>. 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Pareja, Aldo, Giacomo Domeniconi, Jie Chen,
Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, and Charles
E Leisersen. Evolvegcn: Evolving graph convolutional networks for
dynamic graphs. <span style="font-style: italic;">AAAI</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Perez, Ethan, Florian Strub, Harm De Vries,
Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In <span style="font-style: italic;">AAAI</span>, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Pham, Trang, Truyen Tran, and Svetha Venkatesh. "Relational dynamic memory networks." <span style="font-style: italic;">arXiv preprint</span> arXiv:1808.04247 (2018).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Prates, Marcelo, Pedro HC Avelar, Henrique
Lemos, Luis C Lamb, and Moshe Y Vardi. Learning to solve np-complete
problems: A graph neural network for decision tsp. In <span style="font-style: italic;">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 33, pages 4731–4738, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Rabinowitz, Neil C., et al. “Machine theory of mind.” In <span style="font-style: italic;">ICML</span> (2018).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Radford, Alec, Karthik Narasimhan, Tim
Salimans, and Ilya Sutskever. Improving language understand- ing by
generative pre-training, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Ralph Abboud, Ismail Ilkan Ceylan, and
Thomas Lukasiewicz. Learning to reason: Leveraging neural networks for
approximate dnf counting. <span style="font-style: italic;">AAAI</span>, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Ramsauer, Hubert, et al. "Hopfield networks is all you need." <span style="font-style: italic;">arXiv preprint </span>arXiv:2008.02217 (2020).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Rasmus Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In <span style="font-style: italic;">NeurIPS</span>, pages 3368–3378, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Santoro, Adam, David Raposo, David G
Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and
Lillicrap, Tim. A simple neural network module for relational
reasoning. In <span style="font-style: italic;">NIPS</span>, pages 4974–4983, 2017.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Santoro, Adam, Ryan Faulkner, David Raposo,
Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Vinyals,
Oriol, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent
neural networks. <span style="font-style: italic;">NIPS</span>, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Sato, Ryoma, Makoto Yamada, and Hisashi Kashima. Approximation ratios of graph neural networks for combinatorial problems. <span style="font-style: italic;">arXiv preprint</span> arXiv:1905.10261, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Schlag, Imanol and Ju¨ rgen Schmidhuber. Learning to reason with third order tensor products. In <span style="font-style: italic;">Advances in Neural Information Processing Systems</span>, pages 9981–9993, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Seo et al., Dynamic coattention networks for question answering, <span style="font-style: italic;">ICLR</span> 2017 <br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Seo, Minjoon, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. <span style="font-style: italic;">ICLR</span>, 2017.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Sukhbaatar, Sainbayar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. <span style="font-style: italic;">NIPS</span>, 2015.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Sun, Chen, et al. "Videobert: A joint model for video and language representation learning.“ <span style="font-style: italic;">ICCV’19</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Tay, Yi, et al. "Efficient transformers: A survey." a<span style="font-style: italic;">rXiv preprint</span> arXiv:2009.06732 (2020).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I.
“Attention is all you need”. In <span style="font-style: italic;">NIPS</span>, 2017.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Veličković, Petar, and Charles Blundell. "Neural Algorithmic Reasoning." <span style="font-style: italic;">arXiv preprint</span> arXiv:2105.02761 (2021).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Veličković, Petar, Guillem Cucurull,
Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. “Graph
attention networks.”, In <span style="font-style: italic;">ICLR</span>, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Veličković, Petar, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. "Neural Execution of Graph Algorithms." In <span style="font-style: italic;">International Conference on Learning Representations</span>. 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In <span style="font-style: italic;">Advances in Neural Information Processing Systems</span>, pages 2692–2700, 2015.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Weston, J., Bordes, A., Chopra, S., Rush,
A. M., van Merriënboer, B., Joulin, A., &amp; Mikolov, T. (2015).
Towards ai-complete question answering: A set of prerequisite toy
tasks. <span style="font-style: italic;">arXiv preprint</span> arXiv:1502.05698.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Xiong, Caiming, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering. In <span style="font-style: italic;">International Conference on Machine Learning</span>, pages 2397–2406, 2016.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Xu, Keylu, Jingling Li, Mozhi Zhang, Simon
S. Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. "What Can Neural
Networks Reason About?." <span style="font-style: italic;">ICLR</span> 2020 (2020).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Yan, Yujun, Kevin Swersky, Danai Koutra,
Parthasarathy Ranganathan, and Milad Hashemi. "Neural Execution
Engines: Learning to Execute Subroutines." <span style="font-style: italic;">Advances in Neural Information Processing Systems</span> 33 (2020).<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Yang, Zhilin, Zihang Dai, Yiming Yang,
Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:
Generalized autoregressive pretraining for language understanding. In <span style="font-style: italic;">Advances in neural information processing systems</span>, pages 5753–5763, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Yi, Kexin, Chuang Gan, Yunzhu Li, Pushmeet
Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer:
Collision events for video representation and reasoning. <span style="font-style: italic;">arXiv preprint</span> arXiv:1910.01442, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Yoon, KiJung, Renjie Liao, Yuwen Xiong,
Lisa Zhang, Ethan Fetaya, Raquel Urtasun, Richard Zemel, and Xaq
Pitkow. Inference in probabilistic graphical models by graph neural
networks. In 2019 <span style="font-style: italic;">53rd Asilomar Conference on Signals, Systems, and Computers</span>, pages 868–875. IEEE, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Yu, Adams Wei, David Dohan, Minh-Thang
Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. QANet:
Combining Local Convolution with Global Self-Attention for Reading
Compre- hension. <span style="font-style: italic;">ICLR</span>, 2018.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Zellers, Rowan, et al. "From recognition to cognition: Visual commonsense reasoning." CVPR’19.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Zeng, Chengchang, Shaobo Li, Qin Li, Jie
Hu, and Jianjun Hu. A survey on machine reading com- prehension: Tasks,
evaluation metrics, and benchmark datasets. <span style="font-style: italic;">arXiv preprint</span> arXiv:2006.11880, 2020.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Zeng, Kuo-Hao, et al. "Leveraging video descriptions to learn video question answering." <span style="font-style: italic;">AAAI’17</span>.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Zhang, Yuyu, Xinshi Chen, Yuan Yang, Arun
Ramamurthy, Bo Li, Yuan Qi, and Le Song. Can graph neural networks help
logic reasoning? <span style="font-style: italic;">arXiv preprint</span> arXiv:1906.02111, 2019.<br>
          </font></p>
        </li>
        <li>
          <p><font size="5">Zhao, Zhou, et al. "Video question answering via hierarchical dual-level attention network learning." <span style="font-style: italic;">ACL’17</span>.<br>
          </font></p>
        </li>
</ol></td></tr></tbody></table></body></html>