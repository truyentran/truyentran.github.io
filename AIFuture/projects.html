<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>AI Future | Prof Truyen Tran</title>

<meta content="en-us" http-equiv="Content-Language">
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">
<style>
body {
font-family: 'Abel';
font-size: 100px;
}
</style>
<meta name="GENERATOR" content="LyX 2.3.1-1">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<style type="text/css">
/* Layout-provided Styles */
ol.enumerate {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;</style>
<meta name="GENERATOR" content="LyX 2.3.1-1">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<style type="text/css">
/* Layout-provided Styles */
ol.enumerate {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;
}
</style></head>
<body>
<table style="border-collapse: collapse; width: 1000px; height: 1000px;" id="1" border="0" bordercolor="#111111" cellpadding="3" cellspacing="0">
<tbody>
<tr>
<td style="border-width: 1px; border-right: 1px solid; vertical-align: top; background-color: rgb(241, 242, 241);" v="" rowspan="8">
<p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="generated digits" src="figs/deepLearningAI500.png" hspace="0"><br>
</p>
<p align="right">[Source:&nbsp;rdn-consulting]
&nbsp;</p>
<p align="right"> </p>
<p align="right"><big><a href="../index.html">Home</a> <br><a href="index.html">AI Future page</a> <br><a href="#TalksTutorials"></a><a href="#Publications"><br></a></big> </p>
<br>
&nbsp; <br>
</td>
</tr>
<tr>
<td></td>
<td style="background-color: rgb(215, 228, 244);">
<p style="color: rgb(0, 51, 0);"><font style="color: rgb(0, 102, 0); font-weight: bold;" size="+3">&nbsp;AI
Future Projects</font></p>
</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="background-color: rgb(222, 236, 244);"><big><span style="font-weight: bold;">&nbsp;Projects</span><br>
</big>
<table style="text-align: left; width: 100%;" border="0" cellpadding="2" cellspacing="2">
<tbody>
<tr>
<td style="vertical-align: top; width: 50%;"><big>»
<a href="#New_inductive_biases_in_deep_learning">New
inductive biases in deep learning</a></big><br>
<big>» <a href="#Memory_architectures_for_neural_networks">Memory
architectures for neural networks</a></big><br>
<big>» <a href="#Indirection_mechanisms_for_better">Indirection
mechanisms for&nbsp;generalization</a></big><br>
<big>» <a href="#Compositional_reasoning_in_">Compositional
reasoning in vision-language</a></big><br>
<big>» <a href="#Collaborative_priors_for_LLM-powered">Collaborative
priors for LLM multi-agents</a><br>
</big><big>» <a href="#Learning_for_structural_reasoning">Learning for
structural reasoning</a><br>
</big><big>» <a href="#Theory_of_mind_architectures">Theory of mind
architectures</a><br>
</big><big>» <a href="#Efficient_exploration_of_combinatorial">Efficient
exploration of combinatorial spaces</a></big></td>
<td style="vertical-align: top;"><big>» <a href="#Theory_of_mind_in_LLMs_">Theory of mind in LLMs</a></big><br>
<big>» <a href="#Scaling_with_sparse_mixture_of_experts_">Scaling with
sparse mixture of experts</a></big><br>
<big>» <a href="https://theoryinformed-ml.github.io/">Theory-informed
machine learning</a></big><br>
<big>» <a href="#Representing_and_reasoning_over_noisy_">Representing
and reasoning over noisy data</a><br>
</big><big>» <a href="#Structured_reasoning_in_video_">Structured reasoning
in video</a></big><br>
<big>» </big><a href="AI-fundamental.html#Human_behaviour_understanding_in_video"><big>Understanding
human behaviours in video</big></a><br>
<big>» </big><a href="AI-fundamental.html#Visual_question_answering_and_dialog"><big>Visual
question answering</big></a><br>
<big>» </big><a href="#Video_dialog"><big>Video
dialog</big></a></td>
</tr>
</tbody>
</table>
&nbsp;</td>
</tr>
<tr>
<td></td>
</tr>
<tr>
<td></td>
<td style="background-color: rgb(233, 233, 233);"><big><span style="font-weight: bold;">Projects (Old)<br>
</span></big>
<table style="text-align: left; width: 100%;" border="0" cellpadding="2" cellspacing="2">
<tbody>
<tr>
<td style="vertical-align: top; width: 50%;"><big>»
Recomender systems: Random fields</big><br>
<big>» Ordinal choice modelling</big><br>
<big>» Conditional random fields</big><br>
<big>» Advances in Restricted
Boltzmann Machines<br>
</big><big>» Software projects analytics and
automation&nbsp;<br>
</big><big>» Software language models</big></td>
<td style="vertical-align: top;"><big>»
High-dimensional model stability</big><br>
<big>» RNNs for structured data</big><br>
<big>» Advances in representation learning<br>
</big><big>» Understanding GANs<br>
</big><big>» Learning relational structures in
time</big><br>
<big>» Learning to represent episodic data</big></td>
</tr>
</tbody>
</table>
<br>
</td>
</tr>
<tr>
<td style="width: 24px;">&nbsp; &nbsp;
&nbsp;&nbsp;
<p>&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp; <br>
</p>
</td>
<td style="vertical-align: top; width: 90%; background-color: white;"><big><big><o:p></o:p></big></big>
<hr style="width: 100%; height: 2px;"><big>
</big><br>
<p class="MsoNormal" style="text-align: justify; font-weight: bold;"><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="New_inductive_biases_in_deep_learning"></a>New inductive biases in deep learning</span></big></big></p>
<p class="MsoNormal" style="text-align: justify;"><big>We
derive modular networks for regular data such as matrix
and
tensor as well as new data such as graphs and relations. We draw our
architectural inspiration from neuroscience including the columnar
structure of
the neocortex for distributed processing, the thalamus structure for
information routing, working memory for problem solving, and episodic
memory
for integrating information over time.</big></p>
<div style="text-align: center;"><big><big><span><img style="width: 367px; height: 246px;" alt="Column networks" src="figs/column-net.png" v:shapes="_x0000_i1033"><br>
</span></big></big><big><span style="font-style: italic;">Column
Networks, as inspired
by the cortical columns, to solve multi-relational learning</span>.</big><big><br>
</big></div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Memory_architectures_for_neural_networks"></a>Memory
architectures for neural networks<br>
</span></big></big>
<p class="MsoNormal" style="text-align: justify;"><big>Deep
neural networks excel at
function approximation and pattern recognition but fall short on
manipulating
complex, highly dependent systems, rapid contextualisation in new
settings, retaining previously acquired skills, and holding long
conversations. These limitations are possibly due to the lack of an
explicit notion of memory. We design new kinds of memory with more
robust handling of variability,
less
memorization, and stored programs. The memory serves as a central
component in a grand unified cognitive architecture that naturally
supports learning, reasoning, rapid contextualisation and imagination.</big></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 519px; height: 345px;" alt="Variational memory encoder decoder" src="figs/VMED.png" v:shapes="Picture_x0020_2"><br>
</span></big></big><big style="font-style: italic;">Generative models with
variational memory</big></div>
<p class="MsoNormal" style="text-align: justify;"><big style="font-style: italic;"><big><o:p></o:p></big></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big>&nbsp;<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Compositional_reasoning_in_"></a>Compositional
reasoning in vision-language domains<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>Compositionality
is pervasive in nature, language, and our thought processes, enabling
us to understand complex concepts by combining simpler elements.
However, these compositional structures must be uncovered from raw
signals and texts through sophisticated AI methods. Our research
develops neural architectures that learn to identify and manipulate
compositional patterns across visual and linguistic domains. By
integrating computer vision and natural language processing, we model
how basic visual elements combine into objects, scenes, and actions,
while simultaneously capturing how words form phrases, sentences, and
narratives. This compositional understanding enables more robust and
interpretable AI systems capable of human-like reasoning across
modalities. </big><br></div><big>
&nbsp;<br>
<br>
</big>
<div style="text-align: center;"><big><big><span style=""><img style="width: 700px; height: 224px;" alt="A system for Video QA" src="../AI4Science/figs/ProVil.png" v:shapes="Picture_x0020_1"><br>
</span></big></big><big><span style="font-style: italic;">Compositional reasoning over complex queries.</span></big><br>
</div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Learning_for_structural_reasoning"></a>Learning
for relational and causal reasoning<br>
<br>
</span></big></big><div style="text-align: justify;"><big>Relational and
causal structures exist in nature, language and thought processes.
&nbsp; </big><br></div>
<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
</span></big></big>
<div style="text-align: center;"><big><big><span style=""><img style="width: 579px; height: 289px;" alt="Relational Dynamic Memory Network" src="figs/RDMN.png" v:shapes="Picture_x0020_2147"></span></big></big><big><span style="font-style: italic;"><br>
Relational Dynamic Memory
Network, a model for detecting relations between graphical structures.</span></big></div>

<big> </big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;">
</span>
<hr style="width: 100%; height: 2px;"><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;"></span><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Indirection_mechanisms_for_better"></a>Indirection
mechanisms for better generalization<br>
<br>
</span></big></big><div style="text-align: justify;"><big>The capacity of
extreme generalization into completely new
domains in human-level intelligence is strongly connected to the
ability to abstract out the complex world and draw analogies between
seemingly disconnected parts of the world. Here we search for new
computational foundations that support abstraction, including object
discovery, </big><big>relations discovery,&nbsp;
functional programming, indirection, symbolic manupilation
and&nbsp; formulation of analogies.</big><br></div><big>
<br>
</big>
<div style="text-align: center;"><big><big><span><img v:shapes="Picture_x0020_1" src="figs/InLay-IQ-prob.png" alt="A system for Video QA" style="width: 700px; height: 224px;"><br>
</span></big></big><big><span style="font-style: italic;">A system for abstracting out
visual details, focusing on relations
between images via an indirection mechanism. This is capable of solving
IQ problems.</span></big></div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Theory_of_mind_architectures"></a>Theory of mind
architectures<br>
</span></big></big>
<p class="MsoNormal" style="text-align: justify;"><big>Theory
of mind refers to the ability to attribute the mental states of others.
This is a hallmark of human intelligence. This project aims to design
architectures which enable this ability in artificial agents.&nbsp;</big></p>
<p class="MsoNormal" style="text-align: center;"><big><img style="width: 500px; height: 283px;" alt="ToMAGA system." src="figs/ToMAGA.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">A system of multi-agents
equipped with
social psychology.</span></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Efficient_exploration_of_combinatorial"></a>Efficient
exploration of combinatorial spaces<br>
</span></big></big>
<p class="MsoNormal" style="text-align: justify;"><big>Theory
of mind refers to the ability to attribute the mental states of others.
This is a hallmark of human intelligence. This project aims to design
architectures which enable this ability in artificial agents.&nbsp;</big></p>
<p class="MsoNormal" style="text-align: center;"><big><img style="width: 500px; height: 283px;" alt="ToMAGA system." src="figs/ToMAGA.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">A system of multi-agents
equipped with
social psychology.</span></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><br>
<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Collaborative_priors_for_LLM-powered"></a>Collaborative
priors for LLM-powered multi-agents<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>
LLMs
are powerful agents, but they are not pre-designed to work with other
agents. This project aims at developing effective priors to enable
LLMs-powered agents to collaborate to achieve greater long-term team
goals.</big><br></div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
</span></big></big> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Theory_of_mind_in_LLMs_"></a>Theory of mind in
LLMs<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>
LLMs
learn to compress the text and then are tuned to follow instructions.
Their ability to understand others' mental states is not a built-in.
This project aims to analyse this ability, and finds new ways to build
theory of mind in to LLMs.</big><br></div><big>
</big><br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big>&nbsp;</big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
<a name="Scaling_with_sparse_mixture_of_experts_"></a>Scaling
with sparse mixture of experts<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>
Sparse
mixture of experts is a scalable solution to train LLMs. However, much
of its behaviours is still poorly understood. This project aims to shed
light on the matter.</big><br></div><big>
</big><br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Representing_and_reasoning_over_noisy_"></a>Representing
and reasoning over noisy data<br>
<br>
</span></big></big><div style="text-align: justify;"><big>The
world is noisy, and system with thousands of sensors are impossible for
human to understand. This project aims at inventing an unified
representation schemes across sensors of multiple types, and reasoning
mechanism over extended period of time.</big><br></div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><br>
<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Structured_reasoning_in_video_"></a>Structured
reasoning in video<br>
</span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
</span></big></big><div style="text-align: justify;"><big>Video
is highly structured, but the structural information is hidden behind
flat pixels, making it challenging for direct reasoning over video's
events, objects, characters and stories. This project to uncover such
structures and demonstrate the effects on complex video reasoning tasks.</big><br></div><big>
</big><br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big>

<p class="MsoListParagraph" style="margin-left: 18pt; text-align: justify; text-indent: -18pt;"><big><big><span style="font-family: Symbol;"><span style=""><span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: bold; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal; color: rgb(102, 0, 0);"></span></span></span><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Human_behaviour_understanding_in_video"></a>Human
behaviour
understanding in video</span> <o:p style="font-weight: bold; color: rgb(102, 0, 0);"></o:p></big></big></p>
<p class="MsoNormal" style="text-align: justify;"><big>This
project
aims at a deep understanding of human behaviours seen through (fixed
and
moving) videos in various indoor and outdoor contexts. We build new
models of
trajectories and social interactions, inferring past trigger events,
and predict actions and
intention. <o:p></o:p></big></p>
<p class="MsoNormal" style="text-align: justify;"><big><u>Partners</u>:
iCetana</big></p>
<p class="MsoNormal" style="margin-left: 18pt; text-align: justify;"></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 454px; height: 465px;" alt="Anomaly detection with skeleton trajectories" src="figs/skeleton-anomaly.png" v:shapes="Picture_x0020_2086"><br>
</span></big></big><big><span style="font-style: italic;">Detecting anomalies in video
using skeleton trajectories (last row)</span></big><br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big></div>
<p class="MsoListParagraph" style="margin-left: 18pt; text-align: justify; text-indent: -18pt;"><big><big><span style="font-family: Symbol;"><span style=""><span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: bold; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal; color: rgb(102, 0, 0);">
</span></span></span><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Visual_question_answering_and_dialog"></a>Visual
question
answering&nbsp;</span> <o:p style="font-weight: bold; color: rgb(102, 0, 0);"></o:p></big></big></p>
<p style="text-align: justify;" class="MsoNormal"><big>We
study the new cognitive
capability of a system to answer new natural questions about an image
or a
video. This is a powerful way to demonstrate the reasoning capacity,
which
involves linguistic, visual processing and high-level symbols
manipulation
skills. In visual dialog, we build a system having a natural multi-turn
chat
with human about a visual object.</big></p>
<p class="MsoNormal" style="margin-left: 18pt;"></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 568px; height: 207px;" alt="Answering question about video" src="figs/VideoQA-exp.png" v:shapes="Picture_x0020_2085"><br>
</span></big></big><big><span style="font-style: italic;">Answering questions about a
video.</span></big></div>
<p class="MsoCaption" style="text-align: center;" align="center"></p>
<big> </big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;">
</span>
<hr style="width: 100%; height: 2px;"><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;"><br>
</span>
<p class="MsoCaption" style="text-align: center;" align="center"></p>
<div style="text-align: left;"><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Video_dialog"></a>Video dialog</span></big></big></div>
<p class="MsoCaption" style="text-align: center;" align="center"><big><big><span style="font-family: Symbol;"><span style=""><span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: bold; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal; color: rgb(102, 0, 0);"></span></span></span><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big><span style="font-style: italic;"></span><big><o:p></o:p></big></big></p>
<p class="MsoCaption" style="text-align: left;"><big>This
project aims to build a dialog system that can talk about video of
arbitrary length.</big></p>
<p class="MsoCaption" style="text-align: left;"></p><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;"></span><font style="font-weight: bold;" size="5"></font><ol>
</ol>
<ul>
</ul>
<ol>
</ol>
</td>
</tr>
</tbody>
</table>
</body></html>