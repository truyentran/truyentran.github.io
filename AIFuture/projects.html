<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>AI Future | Prof Truyen Tran</title>

<meta content="en-us" http-equiv="Content-Language">
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Abel">
<style>
body {
font-family: 'Abel';
font-size: 100px;
}
</style>
<meta name="GENERATOR" content="LyX 2.3.1-1">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<style type="text/css">
/* Layout-provided Styles */
ol.enumerate {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;</style>
<meta name="GENERATOR" content="LyX 2.3.1-1">
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">
<style type="text/css">
/* Layout-provided Styles */
ol.enumerate {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;
}
</style></head>
<body>
<table style="border-collapse: collapse; width: 1000px; height: 1000px;" id="1" border="0" bordercolor="#111111" cellpadding="3" cellspacing="0">
<tbody>
<tr>
<td style="border-width: 1px; border-right: 1px solid; vertical-align: top; background-color: rgb(241, 242, 241);" v="" rowspan="8">
<p align="right"><img style="border: 2px solid ; width: 200px; height: 200px;" alt="generated digits" src="figs/deepLearningAI500.png" hspace="0"><br>
</p>
<p align="right">[Source:&nbsp;rdn-consulting]
&nbsp;</p>
<p align="right"> </p>
<p align="right"><big><a href="../index.html">Home</a> <br><a href="index.html">AI Future page</a> <br><a href="#TalksTutorials"></a><a href="#Publications"><br></a></big> </p>
<br>
&nbsp; <br>
</td>
</tr>
<tr>
<td></td>
<td style="background-color: rgb(215, 228, 244);">
<p style="color: rgb(0, 51, 0);"><font style="color: rgb(0, 102, 0); font-weight: bold;" size="+3">&nbsp;AI
Future Projects</font></p>
</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td style="background-color: rgb(222, 236, 244);"><big><span style="font-weight: bold;">&nbsp;Projects</span><br>
</big>
<table style="text-align: left; width: 100%;" border="0" cellpadding="2" cellspacing="2">
<tbody>
<tr>
<td style="vertical-align: top; width: 50%;"><big>»
<a href="#New_inductive_biases_in_deep_learning">New
inductive biases in deep learning</a></big><br>
<big>» <a href="#Memory_architectures_for_neural_networks">Memory
architectures for neural networks</a></big><br>
<big>» <a href="#Indirection_mechanisms_for_better">Indirection
mechanisms for&nbsp;generalization</a></big><br>
<big>» <a href="#Compositional_reasoning_in_">Compositional
reasoning in vision-language</a></big><br>
<big>» <a href="#Collaborative_priors_for_LLM-powered">Collaborative
priors for LLM multi-agents</a><br>
</big><big>» <a href="#Learning_for_structural_reasoning">Learning for
structural reasoning</a><br>
</big><big>» <a href="#Theory_of_mind_architectures">Theory of mind
architectures</a><br>
</big><big>» <a href="#Efficient_exploration_of_combinatorial">Efficient
exploration of combinatorial spaces</a></big></td>
<td style="vertical-align: top;"><big>» <a href="#Theory_of_mind_in_LLMs_">Theory of mind in LLMs</a></big><br>
<big>» <a href="#Scaling_with_sparse_mixture_of_experts_">Scaling with
sparse mixture of experts</a></big><br>
<big>» <a href="https://theoryinformed-ml.github.io/">Theory-informed
machine learning</a></big><br>
<big>» <a href="#Representing_and_reasoning_over_noisy_">Representing
and reasoning over noisy data</a><br>
</big><big>» <a href="#Structured_reasoning_in_video_">Structured reasoning
in video</a></big><br>
<big>» </big><a href="AI-fundamental.html#Human_behaviour_understanding_in_video"><big>Understanding
human behaviours in video</big></a><br>
<big>» </big><a href="AI-fundamental.html#Visual_question_answering_and_dialog"><big>Visual
question answering</big></a><br>
<big>» </big><a href="#Video_dialog"><big>Video
dialog</big></a></td>
</tr>
</tbody>
</table>
&nbsp;</td>
</tr>
<tr>
<td></td>
</tr>
<tr>
<td></td>
<td style="background-color: rgb(233, 233, 233);"><big><span style="font-weight: bold;">Projects (Old)<br>
</span></big>
<table style="text-align: left; width: 100%;" border="0" cellpadding="2" cellspacing="2">
<tbody>
<tr>
<td style="vertical-align: top; width: 50%;"><big>»
Recomender systems: Random fields</big><br>
<big>» Ordinal choice modelling</big><br>
<big>» Conditional random fields</big><br>
<big>» Advances in Restricted
Boltzmann Machines<br>
</big><big>» Software projects analytics and
automation&nbsp;<br>
</big><big>» Software language models</big></td>
<td style="vertical-align: top;"><big>»
High-dimensional model stability</big><br>
<big>» RNNs for structured data</big><br>
<big>» Advances in representation learning<br>
</big><big>» Understanding GANs<br>
</big><big>» Learning relational structures in
time</big><br>
<big>» Learning to represent episodic data</big></td>
</tr>
</tbody>
</table>
<br>
</td>
</tr>
<tr>
<td style="width: 24px;">&nbsp; &nbsp;
&nbsp;&nbsp;
<p>&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp; <br>
</p>
</td>
<td style="vertical-align: top; width: 90%; background-color: white;"><big><big><o:p></o:p></big></big>
<hr style="width: 100%; height: 2px;"><big>
</big><br>
<p class="MsoNormal" style="text-align: justify; font-weight: bold;"><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="New_inductive_biases_in_deep_learning"></a>New inductive biases in deep learning</span></big></big></p>
<p class="MsoNormal" style="text-align: justify;"><big>This research
project explores novel architectural designs for neural networks,
drawing direct inspiration from biological neural systems. By studying
the structural organization of the brain, particularly the columnar
architecture of the neocortex and the routing mechanisms of the
thalamus, we develop modular networks optimized for diverse data types
including matrices, tensors, graphs, and relational data. Our approach
integrates key cognitive mechanisms such as working memory for enhanced
problem-solving capabilities and episodic memory for temporal
information integration. This biomimetic framework aims to improve
neural network performance by incorporating proven solutions from
neuroscience, potentially leading to more robust and adaptable AI
systems.</big></p>
<div style="text-align: center;"><big><big><span><img style="width: 367px; height: 246px;" alt="Column networks" src="figs/column-net.png" v:shapes="_x0000_i1033"><br>
</span></big></big><big><span style="font-style: italic;">Column
Networks, as inspired
by the cortical columns, to solve multi-relational learning</span>.</big><big><br>
</big></div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Memory_architectures_for_neural_networks"></a>Memory
architectures for neural networks<br>
</span></big></big>
<p class="MsoNormal" style="text-align: justify;"><big>While current
deep learning systems demonstrate remarkable capabilities in pattern
recognition, they struggle with higher-order cognitive tasks like
complex system manipulation, rapid adaptation, and maintaining coherent
long-term interactions. This research addresses these limitations by
developing novel memory architectures that move beyond simple pattern
matching. Our approach implements explicit memory systems capable of
robust generalization, reduced reliance on rote memorization, and
program-like information storage. This framework forms the foundation
of a comprehensive cognitive architecture that seamlessly integrates
learning, reasoning, and creative processes. By incorporating these
advanced memory mechanisms, we aim to bridge the gap between current AI
capabilities and human-like cognitive flexibility and contextual
understanding.</big></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 519px; height: 345px;" alt="Variational memory encoder decoder" src="figs/VMED.png" v:shapes="Picture_x0020_2"><br>
</span></big></big><big style="font-style: italic;">Generative models with
variational memory</big></div>
<p class="MsoNormal" style="text-align: justify;"><big style="font-style: italic;"><big><o:p></o:p></big></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big>&nbsp;<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Compositional_reasoning_in_"></a>Compositional
reasoning in vision-language domains<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>Compositionality
is pervasive in nature, language, and our thought processes, enabling
us to understand complex concepts by combining simpler elements.
However, these compositional structures must be uncovered from raw
signals and texts through sophisticated AI methods. Our research
develops neural architectures that learn to identify and manipulate
compositional patterns across visual and linguistic domains. By
integrating computer vision and natural language processing, we model
how basic visual elements combine into objects, scenes, and actions,
while simultaneously capturing how words form phrases, sentences, and
narratives. This compositional understanding enables more robust and
interpretable AI systems capable of human-like reasoning across
modalities. </big><br></div><big>
&nbsp;<br>
<br>
</big>
<div style="text-align: center;"><big><big><span style=""><img style="width: 700px; height: 224px;" alt="A system for compositional reasoning" src="../AI4Science/figs/ProVil.png" v:shapes="Picture_x0020_1"><br>
</span></big></big><big><span style="font-style: italic;">Compositional reasoning over complex queries.</span></big><br>
</div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Learning_for_structural_reasoning"></a>Learning
for relational and causal reasoning<br>
<br>
</span></big></big><div style="text-align: justify;"><big>This research
investigates the fundamental role of relational and causal structures
across natural phenomena, linguistic systems, and cognitive processes.
By recognizing that causality and relationships are core organizing
principles in both physical and abstract domains, we develop novel
computational approaches to capture and reason about these structures.
Our work spans multiple levels, from identifying basic causal
mechanisms in natural systems to understanding how relational thinking
shapes language acquisition and human reasoning. Through advanced
machine learning techniques, we aim to create AI systems that can learn
and leverage these inherent structural patterns, leading to more
sophisticated understanding and decision-making capabilities comparable
to human cognitition.&nbsp; </big><br></div>
<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
</span></big></big>
<div style="text-align: center;"><big><big><span style=""><img style="width: 579px; height: 289px;" alt="Relational Dynamic Memory Network" src="figs/RDMN.png" v:shapes="Picture_x0020_2147"></span></big></big><big><span style="font-style: italic;"><br>
Relational Dynamic Memory
Network, a model for detecting relations between graphical structures.</span></big></div>

<big> </big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;">
</span>
<hr style="width: 100%; height: 2px;"><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;"></span><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Indirection_mechanisms_for_better"></a>Indirection
mechanisms for better generalization<br>
<br>
</span></big></big><div style="text-align: justify;"><big>This research
explores fundamental mechanisms enabling human-like generalization and
abstraction in artificial intelligence systems. By investigating how
humans effortlessly transfer knowledge across disparate domains through
analogical reasoning, we develop new computational frameworks that
support sophisticated abstraction capabilities. Our approach
encompasses multiple dimensions: automated discovery of objects and
their relationships, implementation of functional programming
principles, development of indirect reference mechanisms, and
formulation of complex analogies. These components work together to
create AI systems capable of symbolic manipulation and abstract
reasoning, ultimately enabling the kind of extreme generalization
characteristic of human intelligence. This work aims to bridge the gap
between current AI's domain-specific competence and human-level general
intelligence.</big><big></big><br></div><big>
<br>
</big>
<div style="text-align: center;"><big><big><span><img v:shapes="Picture_x0020_1" src="figs/InLay-IQ-prob.png" alt="Analogical reasoning for IQ-test questions" style="width: 700px; height: 224px;"><br>
</span></big></big><big><span style="font-style: italic;">A system for abstracting out
visual details, focusing on relations
between images via an indirection mechanism. This is capable of solving
IQ problems.</span></big></div>
<br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Theory_of_mind_architectures"></a>Theory of mind
architectures<br>
</span></big></big>
<p class="MsoNormal" style="text-align: justify;"><big>This research
develops artificial intelligence systems capable of understanding and
attributing mental states to others—a fundamental aspect of human
social cognition. Drawing from developmental psychology, cognitive
science, and anthropology, we design architectures that enable AI
agents to engage in sophisticated social interactions. Our approach
encompasses multiple innovations: role-learning frameworks for
cooperative agents, guilt-aversion mechanisms to enhance cooperation,
and memory-augmented neural networks for processing long-term social
experiences. Particular emphasis is placed on developing false-belief
understanding, allowing agents to recognize that others may hold
beliefs incongruent with reality. The project aims to create more
socially intelligent AI systems that can effectively collaborate in
team environments.&nbsp;</big></p>
<p class="MsoNormal" style="text-align: center;"><big><img style="width: 500px; height: 283px;" alt="Theory of mind in agents" src="figs/ToMAGA.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">A system of multi-agents
equipped with
social psychology.</span></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Efficient_exploration_of_combinatorial"></a>Efficient
exploration of combinatorial spaces<br>
</span></big></big>
<p class="MsoNormal" style="text-align: justify;"><big>This research
addresses the fundamental challenge of navigating vast combinatorial
search spaces in critical domains including structural design,
materials science, drug discovery, and network optimization. Given the
exponential growth of possible solutions with problem size, traditional
exhaustive search methods become intractable. We develop novel
generative AI approaches that intelligently balance exploration of new
possibilities, exploitation of promising solutions, and maintenance of
solution diversity. Our methods employ advanced sampling strategies and
learning algorithms to efficiently traverse these complex spaces,
enabling practical solutions to previously intractable problems. This
work aims to accelerate discovery and optimization processes across
multiple scientific and engineering domains.</big></p><p class="MsoNormal" style="text-align: justify;"><big></big></p>
<p class="MsoNormal" style="text-align: center;"><big><img style="width: 700px; height: 286px;" alt="Crystal structures generated using Generative AI" src="figs/generated-crystals.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">Crystal structures generated and optimized by several generative AI techniques.</span></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><br>
<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Collaborative_priors_for_LLM-powered"></a>Collaborative
priors for LLM-powered multi-agents<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>This
research develops novel frameworks to enhance collaboration between
Large Language Model (LLM) powered agents, particularly in challenging
social dilemma scenarios. While LLMs demonstrate remarkable individual
capabilities, they lack inherent collaborative mechanisms. We design
specialized priors that guide these agents toward effective team
cooperation and long-term goal achievement. Our approach implements two
key methodologies: strategic prompting and targeted interventions,
leveraging the rich cooperative concepts embedded in LLMs'
pre-training. This framework enables agents to make decisions that
balance individual actions with team objectives, creating more
effective multi-agent systems capable of handling complex social
interactions and collective decision-making scenarios.</big><big></big><br></div>
<br><p class="MsoNormal" style="text-align: center;"><big><img style="width: 700px; height: 248px;" alt="LLM-agent prompted with social priors" src="figs/Prior-guided-LLM-agents.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">LLM-agent prompted with social priors.</span></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
</span></big></big> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Theory_of_mind_in_LLMs_"></a>Theory of mind in
LLMs<br>
</span></big></big><br><big>
</big><table style="text-align: left; width: 100%;" border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="width: 50%; text-align: justify; vertical-align: top;"><big>This research investigates and enhances Large Language Models'
capacity to understand and reason about mental states of others—a
capability not inherently developed through standard training
processes. While LLMs excel at text compression and instruction
following, their ability to model others' beliefs, intentions, and
knowledge states remains limited. We analyze how these models currently
represent and process social understanding, and develop novel
architectures and training approaches to explicitly incorporate theory
of mind capabilities. Our work spans multiple dimensions: mental state
attribution, belief modeling, intention recognition, and social
reasoning. The project aims to create more socially intelligent
language models that can better understand and navigate human
interactions.</big></td><td style="width: 20px;"></td><td><p class="MsoNormal" style="text-align: center;"><big><img style="width: 370px; height: 369px;" alt="Theory of mind with LLMs" src="figs/ToM-LLMs-DALL.E3.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">DALL.E3 illustration of Theory of Mind in LLMs.</span></big></p></td></tr></tbody></table>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big>&nbsp;</big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
<a name="Scaling_with_sparse_mixture_of_experts_"></a>Scaling
with sparse mixture of experts<br>
</span></big></big><big><br></big><div style="text-align: justify;"><big>This
research investigates the fundamental properties and optimization of
Sparse Mixture of Experts (SMoE) architectures for Large Language Model
training. While SMoE offers promising scalability benefits, its
operational dynamics remain incompletely understood. Our project
conducts systematic analysis of critical design elements: token
representation strategies, router indexing mechanisms, and methods to
prevent mode collapse. We examine how different architectural choices
affect model performance, routing efficiency, and computational
resource utilization. Special attention is given to understanding noise
effects on routing decisions and expert specialization. This work aims
to establish theoretical foundations and practical guidelines for
building more efficient and robust SMoE-based language models.<br>&nbsp;</big><br><p class="MsoNormal" style="text-align: center;"><big><img style="width: 700px; height: 248px;" alt="Discrete representation for MoE" src="figs/VQ-MoE.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">Discrete representation for MoE.</span></big></p><br></div><big>
</big><br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Representing_and_reasoning_over_noisy_"></a>Representing
and reasoning over noisy data: Thinking, fast and slow<br>
<br>
</span></big></big><div style="text-align: justify;"><big>This research
develops a unified framework that mimics the "thinking fast and slow"
patterns in human for analyzing complex, multimodal sensor data streams
in large-scale systems. We address the fundamental challenge of
integrating diverse data types—including text, audio, video, and sensor
feeds—into coherent representations that support long-term reasoning.
Our approach combines self-supervised learning with sophisticated
memory architectures to process time-varying, noisy information
streams. The framework enables effective pattern recognition and
prediction across extended timeframes, with applications in cyber
threat detection, situational awareness, and intelligent system
monitoring. By creating scalable methods for handling noisy,
heterogeneous data, we aim to enhance decision-making capabilities in
complex networked environments.</big><br></div>
<big><br><span style="font-weight: bold;">Partners</span>: Australian Department of Defence<br><br><span style="font-weight: bold;">Duration</span>: 2022-2025<br><br></big><p class="MsoNormal" style="text-align: center;"><big><img style="width: 700px; height: 290px;" alt="Discrete representation for MoE" src="figs/UNITED.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">A&nbsp;general framework for thinking fast and slow in dealing with noisy sensors.</span></big></p><hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big><br>
<big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Structured_reasoning_in_video_"></a>Structured
reasoning in video<br>
</span></big></big><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><br>
</span></big></big><div style="text-align: justify;"><big>This research
aims to uncover and leverage inherent structural patterns in video
content, moving beyond pixel-level analysis to enable sophisticated
reasoning about events, objects, and narratives. The project
encompasses three key components: developing human-centric models for
understanding behavioral context, implementing visual abductive
reasoning to explain observed events, and creating predictive
frameworks for future event states. Through these approaches, we
enhance AI systems' ability to comprehend complex human interactions
and temporal relationships in visual scenes. By incorporating
commonsense knowledge and structured reasoning mechanisms, we aim to
bridge the gap between simple pattern recognition and human-like
understanding of visual narratives.</big><br></div><big>
</big><br><p class="MsoNormal" style="text-align: center;"><big><img style="width: 600px; height: 304px;" alt="COMPUTER architecure for human activity recognition" src="figs/COMPUTER.png"></big><big><span style="font-style: italic;"><br>
</span></big></p>
<p class="MsoNormal" style="text-align: center;"><big><span style="font-style: italic;">An architecture to model human activity in context.</span></big></p>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big>

<p class="MsoListParagraph" style="margin-left: 18pt; text-align: justify; text-indent: -18pt;"><big><big><span style="font-family: Symbol;"><span style=""><span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: bold; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal; color: rgb(102, 0, 0);"></span></span></span><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Human_behaviour_understanding_in_video"></a>Human
behaviour
understanding in video</span> <o:p style="font-weight: bold; color: rgb(102, 0, 0);"></o:p></big></big></p>
<p class="MsoNormal" style="text-align: justify;"><big>This research
develops advanced AI systems for comprehensively analyzing human
behavior in video data across diverse environmental contexts. Our
approach integrates multiple levels of analysis: trajectory modeling,
social interaction patterns, causal inference of trigger events, and
prediction of actions and intentions. We incorporate fundamental human
behavioral principles as inductive biases, including goal-directed
behavior, environmental affordances, and commonsense reasoning. The
project culminates in developing a multimodal foundation model that
seamlessly processes video, text, and object information, grounding
abstract knowledge in concrete visual observations. This framework
enables deeper understanding of human behavior in both static and
dynamic camera scenarios.&nbsp;<o:p></o:p></big></p>
<p class="MsoNormal" style="text-align: justify;"><big><u>Partners</u>:
iCetana</big></p>
<p class="MsoNormal" style="margin-left: 18pt; text-align: justify;"></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 454px; height: 465px;" alt="Anomaly detection with skeleton trajectories" src="figs/skeleton-anomaly.png" v:shapes="Picture_x0020_2086"><br>
</span></big></big><big><span style="font-style: italic;">Detecting anomalies in video
using skeleton trajectories (last row)</span></big><br>
<hr style="width: 100%; height: 2px;"> <big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"></span></big></big></div>
<p class="MsoListParagraph" style="margin-left: 18pt; text-align: justify; text-indent: -18pt;"><big><big><span style="font-family: Symbol;"><span style=""><span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: bold; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal; color: rgb(102, 0, 0);">
</span></span></span><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Visual_question_answering_and_dialog"></a>Visual
question
answering&nbsp;</span> <o:p style="font-weight: bold; color: rgb(102, 0, 0);"></o:p></big></big></p>
<p style="text-align: justify;" class="MsoNormal"><big>This research
advances AI systems' ability to comprehend and answer natural language
questions about visual content, bridging low-level pattern recognition
with high-level symbolic reasoning. We develop dynamic computational
architectures that enable iterative reasoning processes, guided by
linguistic queries to analyze visual scenes. The project encompasses
several key innovations: query-specific neural architectures for
spatio-temporal object interaction analysis, frameworks for
understanding complex human-object relationships and events, and novel
multimodal prompting strategies for few-shot learning in Large Vision
Language Models. This work aims to create more sophisticated visual
reasoning systems capable of handling complex, multi-step queries about
visual content.</big></p>
<p class="MsoNormal" style="margin-left: 18pt;"></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 568px; height: 207px;" alt="Answering question about video" src="figs/VideoQA-exp.png" v:shapes="Picture_x0020_2085"><br>
</span></big></big><big><span style="font-style: italic;"><br>Answering questions about a
video.</span></big></div>
<p class="MsoCaption" style="text-align: center;" align="center"></p>
<big> </big><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;">
</span>
<hr style="width: 100%; height: 2px;"><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;"><br>
</span>
<div style="text-align: left;"><big><big><span style="font-weight: bold; color: rgb(102, 0, 0);"><a name="Video_dialog"></a>Video dialog</span></big></big></div>

<p class="MsoCaption" style="text-align: justify;"><big>This research
develops advanced systems for natural conversations about video content
of any duration. We create neural-symbolic architectures that combine
visual understanding with dialogue capabilities, enabling AI to engage
in meaningful discussions about video content. Our approach parses
complex video sequences into structured representations of object
trajectories and their interactions, maintaining dynamic dialogue
states that evolve with conversation. The system employs sophisticated
neural-symbolic reasoning mechanisms to track object relationships
across space and time, while managing conversation history to ensure
contextually coherent responses. By integrating object-oriented
representations with self-attention mechanisms, we enable comprehensive
understanding of long-range temporal dependencies and complex
narratives in video content.&nbsp;</big></p>
<div style="text-align: center;"><big><big><span style=""><img style="width: 700px; height: 205px;" alt="A model for multi-turn video dialog" src="figs/COST-model.png" v:shapes="Picture_x0020_2085"><br>
</span></big></big><big><span style="font-style: italic;"><br>A model for multi-turn video dialog.</span></big></div><p class="MsoCaption" style="text-align: left;">&nbsp;</p><span style="font-size: 11pt; line-height: 107%; font-family: &quot;Calibri&quot;,sans-serif;"></span><font style="font-weight: bold;" size="5"></font><ol>
</ol>
<ul>
</ul>
<ol>
</ol>
</td>
</tr>
</tbody>
</table>
</body></html>